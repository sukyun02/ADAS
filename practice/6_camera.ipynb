{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    " * @file        6_camera.ipynb\n",
    " * @brief       This notebook provides camera data processing and analysis.\n",
    " *\n",
    " * @authors     Jaehwan Lee (idljh5529@gmail.com)\n",
    " *\n",
    " * @date        2025-08-13 Released by AI Lab, Hansung University\n",
    " *\n",
    "-->\n",
    "\n",
    "# 6. Camera\n",
    "\n",
    "이번 실습에서는 자율주행 차량의 핵심 센서인 카메라 데이터 처리에 대해 학습합니다.\n",
    "\n",
    "## 실습 목표\n",
    "1. **OpenCV 기초**: 이미지 로딩, 표시, 기본 처리 방법 학습\n",
    "2. **이미지 전처리**: 노이즈 제거, 대비 조정, 블러링 등 기본 이미지 처리\n",
    "3. **고급 이미지 처리**: 이진화, 모폴로지, 엣지 검출, 윤곽선 검출\n",
    "4. **차선 검출**: 실제 도로 이미지에서 차선 정보를 도출\n",
    "5. **BEV 변환**: 카메라 캘리브레이션 정보를 활용한 Bird's Eye View 변환\n",
    "\n",
    "## 카메라 센서의 중요성\n",
    "\n",
    "### 1. 자율주행에서의 카메라 역할\n",
    "- **차선 인식**: 차량이 주행해야 할 차선 검출\n",
    "- **물체 검출**: 다른 차량, 보행자, 신호등 등 인식\n",
    "- **거리 추정**: 스테레오 비전이나 단안 카메라 기반 거리 측정\n",
    "- **상황 인식**: 교통 표지판, 신호등 상태 등 판단\n",
    "\n",
    "### 2. 이미지 처리의 필요성\n",
    "- **환경 적응**: 다양한 조명 조건(주간, 야간, 흐림 등)에서 안정적 동작\n",
    "- **노이즈 제거**: 센서 노이즈 및 환경적 노이즈 제거\n",
    "- **특징 추출**: 의미 있는 정보(차선, 물체 등) 추출\n",
    "- **실시간 처리**: 빠른 의사결정을 위한 효율적 알고리즘\n",
    "\n",
    "### 3. BEV 변환의 장점\n",
    "- **직관적 표현**: 위에서 내려다본 시점으로 거리 및 방향 파악 용이\n",
    "- **정확한 측정**: 실제 거리 기반의 정확한 측정\n",
    "- **센서 융합**: 다른 센서(LiDAR, 레이더) 데이터와 통합 용이\n",
    "\n",
    "## 실습 데이터\n",
    "- **다양한 예시 이미지**: 이미지 전처리 실습을 위한 기본적인 여러 예시 이미지들\n",
    "- **다양한 환경의 차량 카메라 이미지**: 주간, 야간에서 취득한 차량 카메라 이미지\n",
    "- **도로 이미지**: 차선 검출을 위한 실제 도로 환경\n",
    "- **카메라 캘리브레이션 정보**: 내부/외부 파라미터\n",
    "\n",
    "## 사용할 라이브러리\n",
    "- **OpenCV**: 컴퓨터 비전 및 이미지 처리\n",
    "- **NumPy**: 수치 연산 및 배열 처리\n",
    "- **Matplotlib**: 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부에 정의된 파이썬 모듈(.py 파일)을 수정할 때마다 매번 커널을 재시작하지 않아도 변경 사항이 자동으로 반영되도록 설정\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 사용자 정의 모듈 임포트\n",
    "sys.path.append('./tutlibs/camera')\n",
    "from tutlibs.camera.custom_camera import CameraProcessor\n",
    "\n",
    "print(\"라이브러리 임포트 완료!\")\n",
    "print(\"Python 버전:\", sys.version)\n",
    "print(\"OpenCV 버전:\", cv2.__version__)\n",
    "print(\"NumPy 버전:\", np.__version__)\n",
    "print(\"Matplotlib 버전:\", plt.matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenCV 기초\n",
    "\n",
    "먼저 OpenCV의 기본적인 이미지 처리 방법을 학습합니다.\n",
    "이미지를 로드하고, 다양한 형태로 표시하는 방법을 실습합니다.\n",
    "\n",
    "### 학습 목표\n",
    "- 이미지 파일 로딩 방법\n",
    "- 컬러 공간 변환 (BGR ↔ RGB ↔ Gray)\n",
    "- matplotlib을 이용한 이미지 표시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카메라 처리 객체 생성\n",
    "camera_processor = CameraProcessor()\n",
    "\n",
    "# 단일 이미지 로딩 테스트\n",
    "print(\"=== 단일 이미지 로딩 테스트 ===\")\n",
    "\n",
    "# BGR 형태로 이미지 로딩 (OpenCV 기본)\n",
    "sample_image_bgr = camera_processor.load_image(\"./../data/camera/vehicle/vehicle_camera_4.png\", color_mode='BGR')\n",
    "print(f\"BGR 이미지 형태: {sample_image_bgr.shape}\")\n",
    "\n",
    "# RGB 형태로 이미지 로딩 (matplotlib 호환)\n",
    "sample_image_rgb = camera_processor.load_image(\"./../data/camera/vehicle/vehicle_camera_4.png\", color_mode='RGB')\n",
    "print(f\"RGB 이미지 형태: {sample_image_rgb.shape}\")\n",
    "\n",
    "# 그레이스케일로 이미지 로딩\n",
    "sample_image_gray = camera_processor.load_image(\"./../data/camera/vehicle/vehicle_camera_4.png\", color_mode='GRAY') # TODO: load_image 함수 완성\n",
    "print(f\"그레이스케일 이미지 형태: {sample_image_gray.shape}\")\n",
    "\n",
    "# 이미지들을 함께 표시\n",
    "images = [sample_image_bgr, sample_image_rgb, sample_image_gray]\n",
    "titles = ['BGR Image', 'RGB Image', 'Grayscale Image']\n",
    "camera_processor.display_multiple_images(images, titles, cols=3, figsize=(18, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 환경의 이미지 로딩\n",
    "print(\"=== 다양한 환경의 이미지 로딩 ===\")\n",
    "\n",
    "# 여러 이미지가 저장된 폴더에서 로딩\n",
    "practice_images_folder = \"./../data/camera/practice\"\n",
    "vehicle_images_folder = \"./../data/camera/vehicle\"\n",
    "practice_images = camera_processor.load_multiple_images(practice_images_folder)\n",
    "vehicle_images = camera_processor.load_multiple_images(vehicle_images_folder)\n",
    "\n",
    "print(f\"로딩된 실습 이미지 수: {len(practice_images)}\")\n",
    "print(f\"로딩된 차량 이미지 수: {len(vehicle_images)}\")\n",
    "\n",
    "### 실습 이미지 표시\n",
    "for i, img in enumerate(practice_images):\n",
    "    print(f\"이미지 {i+1}: 크기 {img.shape}, 데이터 타입 {img.dtype}\")\n",
    "\n",
    "if len(practice_images) >= 6:\n",
    "    display_images = practice_images[:6]\n",
    "    titles = ['camera man', 'circuit', 'coins', 'colored chips', 'dandelion seed', 'coins']\n",
    "else:\n",
    "    display_images = practice_images\n",
    "    titles = [f'Image {i+1}' for i in range(len(practice_images))]\n",
    "\n",
    "camera_processor.display_multiple_images(display_images, titles, cols=3, figsize=(15, 10))\n",
    "\n",
    "### 차량 이미지 표시\n",
    "# 각 이미지의 기본 정보 출력\n",
    "for i, img in enumerate(vehicle_images):\n",
    "    print(f\"이미지 {i+1}: 크기 {img.shape}, 데이터 타입 {img.dtype}\")\n",
    "\n",
    "# 다양한 환경의 이미지들을 표시\n",
    "if len(vehicle_images) >= 4:\n",
    "    display_images = vehicle_images[:4]\n",
    "    titles = ['Day Time - Boulevard', 'Night Time - Tollgate', 'Night Time - Ramp', 'Day Time - Tollgate']\n",
    "else:\n",
    "    display_images = vehicle_images\n",
    "    titles = [f'Image {i+1}' for i in range(len(vehicle_images))]\n",
    "\n",
    "camera_processor.display_multiple_images(display_images, titles, cols=2, figsize=(15, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기본 이미지 처리\n",
    "\n",
    "다양한 이미지들을 의미 있는 데이터로 변환하기 위해 여러 이미지 처리 기법들을 실습합니다.\n",
    "\n",
    "### 처리할 항목들\n",
    "1. **RGB to Grayscale**: 컬러 이미지를 그레이스케일로 변환\n",
    "2. **노이즈 추가 및 제거**: 다양한 노이즈 모델링 및 필터링\n",
    "3. **이미지 반전**: 밝기 반전 효과\n",
    "4. **대비 조정**: 명암 대비 및 밝기 조정\n",
    "5. **블러링 & 샤프닝**: 이미지 흐림 및 선명화\n",
    "6. **모폴로지 연산**: 팽창(Dilation) 및 침식(Erosion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 이미지 처리 실습\n",
    "print(\"=== 기본 이미지 처리 실습 ===\")\n",
    "\n",
    "# 실습용 이미지 선택 (마지막 차량 이미지 사용)\n",
    "test_image = vehicle_images[-1].copy()\n",
    "\n",
    "# 1. RGB to Grayscale 변환\n",
    "print(\"1. RGB to Grayscale 변환\")\n",
    "gray_image = xxxxxx(test_image, xxxxxx) # TODO: cv2.cvtColor 사용 & https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html 참고\n",
    "\n",
    "# 결과들을 함께 표시\n",
    "images_to_show = [test_image, gray_image]\n",
    "\n",
    "\n",
    "titles_to_show = [\n",
    "    'Original', 'Grayscale'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(images_to_show, titles_to_show, cols=2, figsize=(18, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 이미지 선택\n",
    "test_image = vehicle_images[1].copy()\n",
    "\n",
    "# 2. 노이즈 추가\n",
    "print(\"2. 노이즈 추가\")\n",
    "noisy_gaussian = camera_processor.add_noise(test_image, 'gaussian', 25) # TODO: add_noise 함수 완성\n",
    "noisy_salt_pepper = camera_processor.add_noise(test_image, 'salt_pepper', 3) # TODO: add_noise 함수 완성\n",
    "noisy_uniform = camera_processor.add_noise(test_image, 'uniform', 70) # TODO: add_noise 함수 완성\n",
    "\n",
    "# 3. 노이즈 제거\n",
    "print(\"3. 노이즈 제거\")\n",
    "denoised_gaussian = cv2.GaussianBlur(noisy_gaussian, (5, 5), 0)\n",
    "denoised_salt_pepper = cv2.medianBlur(noisy_salt_pepper, 3)\n",
    "\n",
    "# 결과들을 함께 표시\n",
    "images_to_show = [test_image, noisy_gaussian, noisy_salt_pepper, noisy_uniform, denoised_gaussian, denoised_salt_pepper]\n",
    "\n",
    "\n",
    "titles_to_show = [\n",
    "    'Original', 'Gaussian Noise', 'Salt & Pepper Noise', 'Uniform Noise', 'Denoised Gaussian', 'Denoised Salt & Pepper'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(images_to_show, titles_to_show, cols=3, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 이미지 선택\n",
    "test_image = practice_images[3].copy()\n",
    "\n",
    "# 4. 이미지 반전\n",
    "print(\"4. 이미지 반전\")\n",
    "inverted_image = xxxxxx # TODO: bit 연산 중, 부정 연산 사용 (https://engineer-mole.tistory.com/237 참고)\n",
    "\n",
    "# 결과들을 함께 표시\n",
    "images_to_show = [test_image, inverted_image]\n",
    "\n",
    "\n",
    "titles_to_show = [\n",
    "    'Original', 'Inverted'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(images_to_show, titles_to_show, cols=2, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 이미지 선택\n",
    "test_image = practice_images[8].copy()\n",
    "\n",
    "# 5. 대비 조정\n",
    "print(\"5. 대비 조정\")\n",
    "high_contrast = cv2.convertScaleAbs(test_image, alpha=1.5, beta=30) # alpha (float): 대비 조정 (1.0은 원본, >1.0은 대비 증가), beta (int): 밝기 조정 (0은 원본, >0은 밝게)\n",
    "low_contrast = cv2.convertScaleAbs(test_image, alpha=0.5, beta=-20) # alpha (float): 대비 조정 (1.0은 원본, <1.0은 대비 감소), beta (int): 밝기 조정 (0은 원본, <0은 어둡게)\n",
    "\n",
    "# 결과들을 함께 표시\n",
    "images_to_show = [test_image, high_contrast, low_contrast]\n",
    "\n",
    "\n",
    "titles_to_show = [\n",
    "    'Original', 'High Contrast', 'Low Contrast'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(images_to_show, titles_to_show, cols=3, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Contrast를 0부터 255로 정규화 - 히스토그램 스트레칭\n",
    "print(\"6. Contrast에 대한 히스토그램 스트레칭\")\n",
    "stretched_image = cv2.normalize(xxxxxx, None, xxxxxx, xxxxxx, cv2.NORM_MINMAX) # TODO: 히스토그램 스트레칭 함수 인자 작성\n",
    "\n",
    "# 결과들을 함께 표시\n",
    "images_to_show = [test_image, stretched_image]\n",
    "\n",
    "titles_to_show = ['Original', 'Stretched']\n",
    "\n",
    "camera_processor.display_multiple_images(images_to_show, titles_to_show, cols=2, figsize=(12, 6))\n",
    "\n",
    "# 원본 이미지의 히스토그램 확인\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(test_image.ravel(), 256, [0, 256])\n",
    "# min, max 값 확인 및 붉은 색으로 선 시각화\n",
    "plt.axvline(x=test_image.min(), color='red', linestyle='--')\n",
    "plt.axvline(x=test_image.max(), color='red', linestyle='--')\n",
    "print(f\"Original Image Min: {test_image.min()}, Max: {test_image.max()}\")\n",
    "plt.title('Original Histogram')\n",
    "\n",
    "# 결과 이미지의 히스토그램 확인\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(stretched_image.ravel(), 256, [0, 256])\n",
    "# min, max 값 확인 및 붉은 색으로 선 시각화\n",
    "plt.axvline(x=stretched_image.min(), color='red', linestyle='--')\n",
    "plt.axvline(x=stretched_image.max(), color='red', linestyle='--')\n",
    "print(f\"Stretched Image Min: {stretched_image.min()}, Max: {stretched_image.max()}\")\n",
    "plt.title('Stretched Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 이미지 선택\n",
    "test_image = practice_images[4].copy()\n",
    "\n",
    "# 7. 블러링/샤프닝 실습\n",
    "print(\"7. 블러링/샤프닝 실습\")\n",
    "\n",
    "# 블러링 및 샤프닝\n",
    "print(\"블러링 및 샤프닝\")\n",
    "blurred_image = camera_processor.blur_image(test_image, kernel_size=15) # TODO: blur_image 함수 완성\n",
    "sharpened_image = camera_processor.sharpen_image(test_image)\n",
    "\n",
    "# 결과 표시\n",
    "noise_removal_images = [test_image, blurred_image, sharpened_image]\n",
    "\n",
    "\n",
    "noise_removal_titles = [\n",
    "    'Original', 'Blurred', 'Sharpened'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(noise_removal_images, noise_removal_titles, cols=3, figsize=(15, 12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 이미지 선택\n",
    "test_image = practice_images[-1].copy()\n",
    "# test_image = practice_images[5].copy() # TODO: 다양한 이미지에 적용\n",
    "# test_image = practice_images[9].copy() # TODO: 다양한 이미지에 적용\n",
    "# test_image = vehicle_images[-2].copy() # TODO: 다양한 이미지에 적용\n",
    "\n",
    "# 모폴로지 연산 실습 (Dilation & Erosion)\n",
    "print(\"=== 모폴로지 연산 실습 ===\")\n",
    "\n",
    "# 그레이스케일 이미지를 이진화하여 모폴로지 연산에 사용\n",
    "binary_image = camera_processor.binarize_image(test_image, method='otsu')\n",
    "\n",
    "# 팽창 및 침식 연산\n",
    "dilated_image = camera_processor.dilate_image(binary_image, kernel_size=5, iterations=2) # 팽창: 이미지(1)를 팽창시키는 연산 - TODO\n",
    "eroded_image = camera_processor.erode_image(binary_image, kernel_size=3, iterations=1) # 침식: 이미지(1)를 깎아내는 연산 - TODO\n",
    "\n",
    "# 다양한 커널 크기로 실험\n",
    "dilated_small = camera_processor.dilate_image(binary_image, kernel_size=2, iterations=1) # TODO\n",
    "dilated_large = camera_processor.dilate_image(binary_image, kernel_size=7, iterations=3) # TODO\n",
    "eroded_small = camera_processor.erode_image(binary_image, kernel_size=2, iterations=1) # TODO\n",
    "\n",
    "morphology_images = [\n",
    "    binary_image,\n",
    "    dilated_small,\n",
    "    dilated_image,\n",
    "    dilated_large,\n",
    "    eroded_small,\n",
    "    eroded_image,\n",
    "    xxxxxx(dilated_small, eroded_small)  # TODO:두 이미지의 차이 보기 - 논리합 사용 (https://engineer-mole.tistory.com/237 참고)\n",
    "]\n",
    "\n",
    "morphology_titles = [\n",
    "    'Binary Original', 'Dilated (2x2, 1 iter)', 'Dilated (5x5, 2 iter)',\n",
    "    'Dilated (7x7, 3 iter)', 'Eroded (2x2, 1 iter)', 'Eroded (3x3, 1 iter)',\n",
    "    'Dilation - Erosion'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(morphology_images, morphology_titles, cols=3, figsize=(18, 12))\n",
    "\n",
    "# 모폴로지 연산의 효과 분석\n",
    "print(f\"\\n=== 모폴로지 연산 효과 분석 ===\")\n",
    "print(f\"원본 이진 이미지 - 흰색 픽셀 수: {np.sum(binary_image == 255)}\")\n",
    "print(f\"팽창 후 - 흰색 픽셀 수: {np.sum(dilated_image == 255)} (증가량: {np.sum(dilated_image == 255) - np.sum(binary_image == 255)})\")\n",
    "print(f\"침식 후 - 흰색 픽셀 수: {np.sum(eroded_image == 255)} (감소량: {np.sum(binary_image == 255) - np.sum(eroded_image == 255)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 고급 이미지 처리\n",
    "\n",
    "이제 더 복잡한 이미지 처리 기법들을 학습합니다. 이 기법들은 차선 검출과 같은 고급 응용에 필수적입니다.\n",
    "\n",
    "### 학습할 기법들\n",
    "1. **이진화 (Binarization)**: 다양한 임계값 설정 방법\n",
    "2. **모폴로지 연산**: Opening, Closing, Gradient 등\n",
    "3. **엣지 검출**: Sobel, Canny 엣지 검출기\n",
    "4. **윤곽선 검출**: 객체의 경계 찾기\n",
    "\n",
    "이러한 기법들은 실제 도로 환경에서 차선, 차량, 표지판 등을 검출하는 데 핵심적인 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 이미지 선택\n",
    "test_image = practice_images[2].copy() # TODO: 다양한 이미지에 적용\n",
    "# test_image = practice_images[7].copy() # TODO: 다양한 이미지에 적용\n",
    "# test_image = vehicle_images[-1].copy() # TODO: 다양한 이미지에 적용\n",
    "gray_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 이진화 실습 - 다양한 방법 비교\n",
    "print(\"=== 이진화 실습 ===\")\n",
    "\n",
    "# 다양한 이진화 방법 적용\n",
    "binary_simple = camera_processor.binarize_image(gray_image, threshold=127, method='simple') # TODO: binarize_image 함수 완성\n",
    "binary_adaptive_mean = camera_processor.binarize_image(gray_image, method='adaptive_mean') # TODO: binarize_image 함수 완성\n",
    "binary_adaptive_gaussian = camera_processor.binarize_image(gray_image, method='adaptive_gaussian') # TODO: binarize_image 함수 완성\n",
    "binary_otsu = camera_processor.binarize_image(gray_image, method='otsu') # TODO: binarize_image 함수 완성\n",
    "\n",
    "# 결과 비교\n",
    "binarization_images = [\n",
    "    gray_image,\n",
    "    binary_simple,\n",
    "    binary_adaptive_mean,\n",
    "    binary_adaptive_gaussian,\n",
    "    binary_otsu,\n",
    "    cv2.bitwise_and(gray_image, binary_otsu)  # 원본에 마스크 적용\n",
    "]\n",
    "\n",
    "binarization_titles = [\n",
    "    'Original Grayscale', 'Simple (T=127)', 'Adaptive Mean',\n",
    "    'Adaptive Gaussian', 'Otsu', 'Otsu Applied to Original'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(binarization_images, binarization_titles, cols=3, figsize=(18, 12))\n",
    "\n",
    "# 각 방법의 이진화 비율 계산\n",
    "print(f\"\\n=== 이진화 결과 분석 ===\")\n",
    "total_pixels = gray_image.shape[0] * gray_image.shape[1]\n",
    "print(f\"Simple 이진화 - 흰색 비율: {np.sum(binary_simple == 255) / total_pixels * 100:.1f}%\")\n",
    "print(f\"Adaptive Mean - 흰색 비율: {np.sum(binary_adaptive_mean == 255) / total_pixels * 100:.1f}%\")\n",
    "print(f\"Adaptive Gaussian - 흰색 비율: {np.sum(binary_adaptive_gaussian == 255) / total_pixels * 100:.1f}%\")\n",
    "print(f\"Otsu - 흰색 비율: {np.sum(binary_otsu == 255) / total_pixels * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고급 모폴로지 연산 실습\n",
    "print(\"=== 고급 모폴로지 연산 실습 ===\")\n",
    "\n",
    "# 이진 이미지에 다양한 모폴로지 연산 적용\n",
    "morph_opening = camera_processor.morphology_operations(binary_otsu, 'opening', 5) # TODO: morphology_operations 함수 완성\n",
    "morph_closing = camera_processor.morphology_operations(binary_otsu, 'closing', 5) # TODO: morphology_operations 함수 완성\n",
    "morph_gradient = camera_processor.morphology_operations(binary_otsu, 'gradient', 3) # TODO: morphology_operations 함수 완성\n",
    "morph_tophat = camera_processor.morphology_operations(binary_otsu, 'tophat', 9) # TODO: morphology_operations 함수 완성\n",
    "morph_blackhat = camera_processor.morphology_operations(binary_otsu, 'blackhat', 9) # TODO: morphology_operations 함수 완성\n",
    "\n",
    "# 결과 비교\n",
    "morphology_advanced_images = [\n",
    "    binary_otsu,\n",
    "    morph_opening,\n",
    "    morph_closing,\n",
    "    morph_gradient,\n",
    "    morph_tophat,\n",
    "    morph_blackhat\n",
    "]\n",
    "\n",
    "morphology_advanced_titles = [\n",
    "    'Original Binary', 'Opening', 'Closing',\n",
    "    'Gradient', 'Top Hat', 'Black Hat'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(morphology_advanced_images, morphology_advanced_titles, cols=3, figsize=(18, 12))\n",
    "\n",
    "# 각 연산의 특징 설명\n",
    "print(f\"\\n=== 모폴로지 연산 특징 ===\")\n",
    "print(\"Opening: 노이즈 제거, 작은 객체 제거\")\n",
    "print(\"Closing: 구멍 메우기, 객체 연결\")\n",
    "print(\"Gradient: 엣지 검출, 객체 경계\")\n",
    "print(\"Top Hat: 밝은 영역 강조\")\n",
    "print(\"Black Hat: 어두운 영역 강조\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobel filter\n",
    "\n",
    "<img src=\"../resources/ch6/sobel_filter.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엣지 검출 실습 (Sobel & Canny)\n",
    "print(\"=== 엣지 검출 실습 ===\")\n",
    "\n",
    "# Sobel 엣지 검출\n",
    "sobel_x, sobel_y, sobel_combined = camera_processor.edge_detection_sobel(test_image) # TODO: edge_detection_sobel 함수 완성\n",
    "\n",
    "# Canny 엣지 검출 (다양한 임계값으로)\n",
    "canny_50_150 = camera_processor.edge_detection_canny(test_image, 50, 150) # TODO: edge_detection_canny 함수 완성\n",
    "canny_100_200 = camera_processor.edge_detection_canny(test_image, 100, 200) # TODO: edge_detection_canny 함수 완성\n",
    "canny_30_100 = camera_processor.edge_detection_canny(test_image, 30, 100) # TODO: edge_detection_canny 함수 완성\n",
    "\n",
    "# 결과 비교\n",
    "edge_images = [\n",
    "    gray_image,\n",
    "    sobel_x,\n",
    "    sobel_y,\n",
    "    sobel_combined,\n",
    "    canny_30_100,\n",
    "    canny_50_150,\n",
    "    canny_100_200,\n",
    "    cv2.bitwise_or(sobel_combined, canny_50_150)  # Sobel + Canny 결합\n",
    "]\n",
    "\n",
    "edge_titles = [\n",
    "    'Original Gray', 'Sobel X', 'Sobel Y', 'Sobel Combined',\n",
    "    'Canny (30,100)', 'Canny (50,150)', 'Canny (100,200)', 'Sobel + Canny'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(edge_images, edge_titles, cols=4, figsize=(20, 10))\n",
    "\n",
    "# 엣지 검출 결과 분석\n",
    "print(f\"\\n=== 엣지 검출 결과 분석 ===\")\n",
    "print(f\"Sobel Combined - 엣지 픽셀 수: {np.sum(sobel_combined > 0)}\")\n",
    "print(f\"Canny (50,150) - 엣지 픽셀 수: {np.sum(canny_50_150 > 0)}\")\n",
    "print(f\"Canny (100,200) - 엣지 픽셀 수: {np.sum(canny_100_200 > 0)}\")\n",
    "print(f\"Canny (30,100) - 엣지 픽셀 수: {np.sum(canny_30_100 > 0)}\")\n",
    "\n",
    "print(\"\\n임계값이 낮을수록 더 많은 엣지가 검출되지만 노이즈도 증가합니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윤곽선 검출 실습\n",
    "print(\"=== 윤곽선 검출 실습 ===\")\n",
    "\n",
    "# 이진 이미지에서 윤곽선 검출\n",
    "contours, contour_image = camera_processor.detect_contours(binary_otsu) # TODO: detect_contours 함수 완성\n",
    "\n",
    "# 정리된 이진 이미지에서 윤곽선 검출\n",
    "clean_binary = camera_processor.morphology_operations(binary_otsu, 'opening', 3) # TODO: morphology_operations 함수 완성\n",
    "contours_clean, contour_image_clean = camera_processor.detect_contours(clean_binary) # TODO: detect_contours 함수 완성\n",
    "\n",
    "print(f\"원본 이진 이미지에서 검출된 윤곽선 수: {len(contours)}\")\n",
    "print(f\"정리된 이진 이미지에서 검출된 윤곽선 수: {len(contours_clean)}\")\n",
    "\n",
    "# 큰 윤곽선만 필터링\n",
    "area_threshold = 100\n",
    "large_contours = [cnt for cnt in contours_clean if cv2.contourArea(cnt) > xxxxxx] # TODO: 윤곽선 면적 필터링\n",
    "print(f\"큰 윤곽선 (면적 > 100) 개수: {len(large_contours)}\")\n",
    "\n",
    "# 큰 윤곽선만 그리기\n",
    "large_contour_image = cv2.cvtColor(clean_binary, xxxxxx) # TODO: 그레이스케일 이미지를 3채널로 변환\n",
    "cv2.drawContours(large_contour_image, large_contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# 윤곽선 분석 (면적, 둘레, 경계 사각형)\n",
    "contour_analysis_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB).copy()\n",
    "# for i, contour in enumerate(large_contours[:10]):  # 상위 10개만\n",
    "for i, contour in enumerate(contours[:10]):  # 상위 10개만\n",
    "    # 면적과 둘레 계산\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    # 경계 사각형\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(contour_analysis_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # 정보 텍스트\n",
    "    cv2.putText(contour_analysis_image, f'A:{int(area)}', (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "# 결과 표시\n",
    "contour_images = [\n",
    "    binary_otsu,\n",
    "    cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB),\n",
    "    clean_binary,\n",
    "    cv2.cvtColor(contour_image_clean, cv2.COLOR_BGR2RGB),\n",
    "    cv2.cvtColor(large_contour_image, cv2.COLOR_BGR2RGB),\n",
    "    contour_analysis_image\n",
    "]\n",
    "\n",
    "contour_titles = [\n",
    "    'Original Binary', 'All Contours', 'Cleaned Binary',\n",
    "    'Cleaned Contours', 'Large Contours Only', 'Contour Analysis'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(contour_images, contour_titles, cols=3, figsize=(18, 12))\n",
    "\n",
    "# 윤곽선 통계\n",
    "if large_contours:\n",
    "    areas = [cv2.contourArea(cnt) for cnt in large_contours]\n",
    "    print(f\"\\n=== 윤곽선 통계 ===\")\n",
    "    print(f\"가장 큰 윤곽선 면적: {max(areas):.1f}\")\n",
    "    print(f\"평균 윤곽선 면적: {np.mean(areas):.1f}\")\n",
    "    print(f\"윤곽선 면적 범위: {min(areas):.1f} ~ {max(areas):.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 차선 검출 (Lane Detection)\n",
    "\n",
    "이제 앞서 학습한 이미지 처리 기법들을 활용하여 실제 도로 이미지에서 차선을 검출하는 파이프라인을 구현합니다.\n",
    "\n",
    "### 차선 검출 파이프라인\n",
    "1. **전처리**: 그레이스케일 변환, 노이즈 제거\n",
    "2. **엣지 검출**: Canny 엣지 검출기 적용\n",
    "3. **관심 영역 설정**: 도로 영역만 마스킹\n",
    "4. **직선 검출**: 허프 변환(Hough Transform)으로 직선 찾기\n",
    "5. **후처리**: 검출된 직선을 원본 이미지에 오버레이\n",
    "\n",
    "### 학습자 실습 목표\n",
    "- 각 단계별로 파라미터를 조정해가며 최적의 결과 도출\n",
    "- 다양한 조명 조건에서의 차선 검출 성능 비교\n",
    "- 차선 검출 알고리즘의 한계점 및 개선 방안 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 차선 검출 실습\n",
    "print(\"=== 기본 차선 검출 실습 ===\")\n",
    "\n",
    "# 각 이미지에 대해 차선 검출 적용\n",
    "for i, image in enumerate(vehicle_images[-8:]):\n",
    "    print(f\"\\n--- 이미지 {i+1} 차선 검출 ---\")\n",
    "    \n",
    "    # 기본 차선 검출 수행\n",
    "    processed_edges, lane_result, results = camera_processor.lane_detection_pipeline(image) # TODO: lane_detection_pipeline 함수 완성\n",
    "    \n",
    "    # 결과 표시\n",
    "    results = [image, processed_edges, lane_result]\n",
    "    \n",
    "    titles = [f'Original Image {i+1}', f'Processed Edges {i+1}', f'Lane Detection {i+1}']\n",
    "    \n",
    "    camera_processor.display_multiple_images(results, titles, cols=3, figsize=(18, 6))\n",
    "    \n",
    "    # 간단한 성능 분석\n",
    "    edge_pixels = np.sum(processed_edges > 0)\n",
    "    total_pixels = processed_edges.shape[0] * processed_edges.shape[1]\n",
    "    edge_ratio = edge_pixels / total_pixels * 100\n",
    "    \n",
    "    print(f\"엣지 픽셀 비율: {edge_ratio:.2f}%\")\n",
    "    print(f\"관심 영역 크기: {processed_edges.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계별 차선 검출 파이프라인 분석\n",
    "print(\"=== 단계별 차선 검출 파이프라인 분석 ===\")\n",
    "\n",
    "# 첫 번째 이미지로 상세 분석\n",
    "analysis_image = vehicle_images[2].copy()\n",
    "\n",
    "print(\"차선 검출 파이프라인의 각 단계를 분석합니다...\")\n",
    "\n",
    "# 단계별 처리 결과 획득\n",
    "processed_edges, lane_result, pipeline_results = camera_processor.lane_detection_pipeline(analysis_image) # TODO: lane_detection_pipeline 함수 완성\n",
    "\n",
    "# 각 단계 결과를 순서대로 표시\n",
    "step_images = []\n",
    "step_titles = []\n",
    "\n",
    "for step_name, step_image in pipeline_results.items():\n",
    "    if step_name == 'original':\n",
    "        step_images.append(cv2.cvtColor(step_image, cv2.COLOR_BGR2RGB))\n",
    "    elif step_name in ['gray', 'blurred', 'edges', 'masked']:\n",
    "        step_images.append(step_image)\n",
    "    elif step_name in ['lines', 'final']:\n",
    "        step_images.append(cv2.cvtColor(step_image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # 제목을 아래와 같이 변경\n",
    "    title_mapping = {\n",
    "        'original': '1. original image',\n",
    "        'gray': '2. gray image',\n",
    "        'blurred': '3. blurred image',\n",
    "        'edges': '4. edges image',\n",
    "        'masked': '5. masked image',\n",
    "        'lines': '6. lines image',\n",
    "        'final': '7. final image'\n",
    "    }\n",
    "    step_titles.append(title_mapping.get(step_name, step_name))\n",
    "\n",
    "# 모든 단계를 하나의 그리드로 표시\n",
    "camera_processor.display_multiple_images(step_images, step_titles, cols=4, figsize=(20, 12))\n",
    "\n",
    "# 각 단계별 통계 정보\n",
    "print(f\"\\n=== 각 단계별 분석 ===\")\n",
    "print(f\"원본 이미지 크기: {pipeline_results['original'].shape}\")\n",
    "print(f\"그레이스케일 이미지 평균 밝기: {np.mean(pipeline_results['gray']):.1f}\")\n",
    "print(f\"Canny 엣지 검출된 픽셀 수: {np.sum(pipeline_results['edges'] > 0)}\")\n",
    "print(f\"마스킹 후 엣지 픽셀 수: {np.sum(pipeline_results['masked'] > 0)}\")\n",
    "\n",
    "# 마스킹 효과 분석\n",
    "masking_efficiency = np.sum(pipeline_results['masked'] > 0) / np.sum(pipeline_results['edges'] > 0) * 100\n",
    "print(f\"마스킹 효율성: {masking_efficiency:.1f}% (관심 영역에 집중)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BEV 변환 (Bird's Eye View / Inverse Perspective Mapping)\n",
    "\n",
    "카메라 캘리브레이션 정보를 활용하여 원근감이 있는 카메라 이미지를 위에서 내려다본 시점(BEV)으로 변환합니다.\n",
    "\n",
    "### BEV 변환의 수학적 배경\n",
    "\n",
    "#### 1. 카메라 모델\n",
    "카메라는 핀홀 모델로 근사할 수 있으며, 월드 좌표계의 점 **P_w**가 이미지 좌표계의 점 **p**로 투영되는 과정은 다음과 같습니다:\n",
    "\n",
    "**p = K [R|t] P_w**\n",
    "\n",
    "여기서:\n",
    "- **K**: 내부 파라미터 행렬 (Intrinsic Matrix)\n",
    "- **[R|t]**: 외부 파라미터 행렬 (Extrinsic Matrix)\n",
    "- **R**: 회전 행렬 (3×3)\n",
    "- **t**: 평행이동 벡터 (3×1)\n",
    "\n",
    "#### 2. 내부 파라미터 행렬 K\n",
    "```\n",
    "K = [fx  0  cx]\n",
    "    [0  fy  cy]\n",
    "    [0   0   1]\n",
    "```\n",
    "- **fx, fy**: 초점 거리 (픽셀 단위)\n",
    "- **cx, cy**: 주점 (Principal Point) 좌표\n",
    "\n",
    "#### 3. BEV 변환 원리\n",
    "지면 위의 점들 (Z = 0)에 대해:\n",
    "1. **이미지 → 월드**: 지면 조건을 이용한 역투영\n",
    "2. **월드 → BEV**: 직교 투영으로 위에서 내려다본 뷰 생성\n",
    "\n",
    "#### 4. 지면 역투영 수식\n",
    "이미지 점 **(u, v)**가 지면 위의 점 **(X, Y, 0)**에 대응될 때:\n",
    "\n",
    "**λ = (Z_ground + R⁻¹[2,:]·t) / (R⁻¹[2,:]·K⁻¹·[u,v,1]ᵀ)**\n",
    "\n",
    "**[X, Y, Z]ᵀ = R⁻¹(λ·K⁻¹·[u,v,1]ᵀ - t)**\n",
    "\n",
    "### 실습 목표\n",
    "- 카메라 캘리브레이션 파라미터 설정\n",
    "- BEV 변환 행렬 생성\n",
    "- 원본 이미지를 BEV로 변환\n",
    "- BEV 이미지에서 차선 검출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카메라 캘리브레이션 파라미터 설정\n",
    "print(\"=== 카메라 캘리브레이션 파라미터 설정 ===\")\n",
    "\n",
    "# 내부 파라미터 행렬 (Intrinsic Matrix)\n",
    "intrinsic_matrix = np.array([\n",
    "    [1085.674464197818,   0.0, 970.8083862917935],  # fx, 0, cx\n",
    "    [  0.0, 1085.346652238244, 571.7330860257647],  # 0, fy, cy  \n",
    "    [  0.0,   0.0,   1.0]   # 0, 0, 1\n",
    "], dtype=np.float32)\n",
    "\n",
    "# 왜곡 계수 (Distortion Coefficients)\n",
    "distortion_coeffs = np.array([-0.272997037779982, 0.080290395208785, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "# 외부 파라미터 - 차량 뒤축 아래 바닥을 기준으로 설정\n",
    "# 카메라가 차량 앞쪽에 장착되어 있음\n",
    "camera_height = 1.8  # 카메라 높이 (미터)\n",
    "camera_forward = 1.4  # 차량 뒤축에서 카메라까지의 전방 거리 (미터)\n",
    "\n",
    "# 회전 행렬 - 차량 좌표계에서의 카메라 각도 설정\n",
    "rotation_matrix = np.array([\n",
    "    [ -0.0174524,  0.0000000,  0.9998477],\n",
    "    [ -0.9998477, -0.0000000, -0.0174524],\n",
    "    [ 0.0000000, -1.0000000,  0.0000000 ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "tilt_angle_pitch = np.deg2rad(90)  # 90도를 라디안으로 변환\n",
    "rotation_matrix_pitch = np.array([\n",
    "    [np.cos(tilt_angle_pitch),  0.0, np.sin(tilt_angle_pitch)],\n",
    "    [0.0,  1.0, 0.0],\n",
    "    [-np.sin(tilt_angle_pitch), 0.0, np.cos(tilt_angle_pitch)]\n",
    "], dtype=np.float32)\n",
    "\n",
    "tilt_angle_yaw = -np.deg2rad(90)  # 90도를 라디안으로 변환\n",
    "rotation_matrix_yaw = np.array([\n",
    "    [np.cos(tilt_angle_yaw),  -np.sin(tilt_angle_yaw), 0.0],\n",
    "    [np.sin(tilt_angle_yaw),  np.cos(tilt_angle_yaw), 0.0],\n",
    "    [0.0,  0.0, 1.0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "rotation_matrix = rotation_matrix @ rotation_matrix_pitch\n",
    "rotation_matrix = rotation_matrix @ rotation_matrix_yaw\n",
    "\n",
    "# 평행이동 벡터 - 차량 뒤축 기준 좌표계에서 카메라 위치\n",
    "translation_vector = np.array([0.0, camera_forward, camera_height], dtype=np.float32)\n",
    "\n",
    "print(\"내부 파라미터 행렬 K:\")\n",
    "print(intrinsic_matrix)\n",
    "print(f\"\\n카메라 위치: 전방 {camera_forward}m, 높이 {camera_height}m\")\n",
    "\n",
    "# 카메라 처리기에 파라미터 설정\n",
    "camera_processor.setup_camera_params(\n",
    "    intrinsic_matrix, \n",
    "    distortion_coeffs, \n",
    "    rotation_matrix, \n",
    "    translation_vector\n",
    ")\n",
    "\n",
    "# 카메라 파라미터 검증\n",
    "print(\"\\n=== 카메라 파라미터 검증 ===\")\n",
    "print(f\"초점 거리: fx={intrinsic_matrix[0,0]:.1f}, fy={intrinsic_matrix[1,1]:.1f}\")\n",
    "print(f\"주점: cx={intrinsic_matrix[0,2]:.1f}, cy={intrinsic_matrix[1,2]:.1f}\")\n",
    "print(f\"회전 행렬 행렬식: {np.linalg.det(rotation_matrix):.3f} (1에 가까워야 함)\")\n",
    "print(f\"변환 벡터 크기: {np.linalg.norm(translation_vector):.3f}m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEV 변환 실습\n",
    "print(\"=== BEV (Bird's Eye View) 변환 실습 ===\")\n",
    "\n",
    "# 마지막 자동차 실습 이미지로 BEV 변환 수행\n",
    "bev_test_image = vehicle_images[-1].copy()\n",
    "\n",
    "print(\"1. 원본 이미지 왜곡 보정\")\n",
    "# 왜곡 보정\n",
    "undistorted_image = camera_processor.undistort_image(bev_test_image) # TODO: undistort_image 함수 완성\n",
    "\n",
    "print(\"2. BEV 변환 행렬 생성\")\n",
    "# BEV 변환 파라미터 설정\n",
    "bev_width = 15.0      # BEV 이미지의 실제 폭 (미터)\n",
    "bev_height = 8.0    # BEV 이미지의 실제 높이 (미터)  \n",
    "bev_resolution = 0.02  # BEV 해상도 (미터/픽셀)\n",
    "\n",
    "# BEV 변환 행렬 생성\n",
    "transform_matrix, bev_size = camera_processor.create_bev_transform(\n",
    "    bev_test_image.shape[:2], bev_width, bev_height, bev_resolution # TODO: create_bev_transform 함수 완성\n",
    ")\n",
    "\n",
    "print(\"3. BEV 변환 적용\")\n",
    "# BEV 변환 적용\n",
    "bev_image = camera_processor.apply_bev_transform(undistorted_image, xxxxxx, xxxxxx) # TODO: apply_bev_transform 함수 완성\n",
    "\n",
    "# 결과 비교\n",
    "bev_comparison_images = [bev_test_image, undistorted_image, bev_image]\n",
    "\n",
    "bev_comparison_titles = ['Original', 'Undistorted', 'BEV']\n",
    "\n",
    "camera_processor.display_multiple_images(bev_comparison_images, bev_comparison_titles, cols=3, figsize=(18, 8))\n",
    "\n",
    "print(f\"\\n=== BEV 변환 결과 분석 ===\")\n",
    "print(f\"원본 이미지 크기: {bev_test_image.shape[:2]}\")\n",
    "print(f\"BEV 이미지 크기: {bev_image.shape[:2]}\")\n",
    "print(f\"BEV 실제 영역: {bev_width}m × {bev_height}m\")\n",
    "print(f\"BEV 해상도: {bev_resolution}m/pixel\")\n",
    "print(f\"픽셀당 실제 크기: {bev_resolution*100:.0f}cm × {bev_resolution*100:.0f}cm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEV 이미지에서 차선 검출\n",
    "print(\"=== BEV 이미지에서 차선 검출 ===\")\n",
    "\n",
    "print(\"BEV 이미지에서 차선을 검출합니다...\")\n",
    "\n",
    "# BEV 이미지에서 차선 검출\n",
    "bev_lane_binary, bev_lane_result = camera_processor.detect_lanes_bev(bev_image) # TODO: detect_lanes_bev 함수 완성\n",
    "\n",
    "# 결과 표시\n",
    "bev_lane_images = [\n",
    "    bev_image,\n",
    "    bev_lane_binary,\n",
    "    bev_lane_result\n",
    "]\n",
    "\n",
    "# Canny 엣지 검출\n",
    "canny_edges = xxxxxx(xxxxxx, 50, 150) # TODO: Canny 함수 사용 - cv2.Canny\n",
    "\n",
    "bev_lane_images.append(cv2.cvtColor(canny_edges, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# 허프 변환으로 직선 검출\n",
    "hough_threshold = xxxxxx\n",
    "hough_min_line_length = xxxxxx\n",
    "hough_max_line_gap = xxxxxx\n",
    "lines = xxxxxx(canny_edges, 1, np.pi/180, threshold=hough_threshold, \n",
    "                        minLineLength=hough_min_line_length, maxLineGap=hough_max_line_gap) # TODO: 허프 변환 함수 사용 - cv2.HoughLinesP\n",
    "\n",
    "# 검출된 직선을 원본 이미지에 그리기\n",
    "line_image = np.zeros_like(bev_image)\n",
    "if lines is not None:\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        cv2.line(line_image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "\n",
    "bev_lane_images.append(cv2.cvtColor(line_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "bev_lane_titles = [\n",
    "    'BEV original', 'BEV binary', 'BEV lane detection', 'BEV lane detection (Hough)'\n",
    "]\n",
    "\n",
    "camera_processor.display_multiple_images(bev_lane_images, bev_lane_titles, cols=3, figsize=(18, 8))\n",
    "\n",
    "\n",
    "# BEV 차선 검출 성능 분석\n",
    "lane_pixels = np.sum(bev_lane_binary == 255)\n",
    "total_bev_pixels = bev_lane_binary.shape[0] * bev_lane_binary.shape[1]\n",
    "lane_coverage = lane_pixels / total_bev_pixels * 100\n",
    "\n",
    "print(f\"\\n=== BEV 차선 검출 분석 ===\")\n",
    "print(f\"검출된 차선 픽셀 수: {lane_pixels}\")\n",
    "print(f\"차선 픽셀 비율: {lane_coverage:.2f}%\")\n",
    "print(f\"실제 차선 면적: {lane_pixels * (bev_resolution**2):.2f} m²\")\n",
    "\n",
    "# 차선 폭 추정 (간단한 방법)\n",
    "# 각 행에서 차선 픽셀의 분포를 분석\n",
    "row_lane_counts = np.sum(bev_lane_binary == 255, axis=1)\n",
    "avg_lane_width_pixels = np.mean(row_lane_counts[row_lane_counts > 0]) if np.any(row_lane_counts > 0) else 0\n",
    "avg_lane_width_meters = avg_lane_width_pixels * bev_resolution\n",
    "\n",
    "print(f\"평균 검출 차선 폭: {avg_lane_width_pixels:.1f} 픽셀 ({avg_lane_width_meters:.2f} m)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adas_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
