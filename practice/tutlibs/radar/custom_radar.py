"""Radar Practice"""
"""
 * @file        custom_radar.py
 * @brief       Custom Radar practice algorithms for radar data processing
 * 
 * @authors     Jaehwan Lee (idljh5529@gmail.com)          
 *
 * @date        2025-08-12 Released by AI Lab, Hanyang University
 * 
"""
import numpy as np
import pandas as pd
import open3d as o3d
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
import glob
import os
from typing import List, Dict, Tuple, Optional, Union, Any
import time
from scipy.spatial.distance import cdist
from sklearn.cluster import DBSCAN
import copy

###############################################################################
# 1. Radar Data Loader
class RadarDataLoader:
    """ë ˆì´ë” í¬ì¸íŠ¸í´ë¼ìš°ë“œ ë°ì´í„° ë¡œë”© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.point_clouds = []
        self.timestamps = []
        self.vehicle_speeds = []
        self.point_velocities = []  # ê° í¬ì¸íŠ¸ì˜ ë„í”ŒëŸ¬ ì†ë„ ì €ì¥
        
    def load_radar_pcd_files(self, pcd_folder_path: str, pattern: str = "*.pcd") -> List[o3d.geometry.PointCloud]:
        """
        ë ˆì´ë” PCD íŒŒì¼ë“¤ì„ ë¡œë”©
        
        Args:
            pcd_folder_path: PCD íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ
            pattern: íŒŒì¼ íŒ¨í„´ (ê¸°ë³¸ê°’: "*.pcd")
            
        Returns:
            ë¡œë”©ëœ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ë¦¬ìŠ¤íŠ¸
        """
        pcd_files = sorted(glob.glob(os.path.join(pcd_folder_path, pattern)))
        
        if not pcd_files:
            print(f"âš ï¸ {pcd_folder_path}ì—ì„œ PCD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._generate_simulation_radar_data()
        
        print(f"ğŸ“¡ {len(pcd_files)}ê°œì˜ ë ˆì´ë” PCD íŒŒì¼ì„ ë¡œë”© ì¤‘...")
        
        for i, pcd_file in enumerate(pcd_files):
            try:
                # ì»¤ìŠ¤í…€ íŒŒì„œë¡œ velocity field í¬í•¨í•˜ì—¬ ë¡œë”©
                pcd, velocities = self._load_pcd_with_velocity(pcd_file)
                if len(pcd.points) > 0:
                    self.point_clouds.append(pcd)
                    self.point_velocities.append(velocities)
                    self.timestamps.append(i * 0.1)  # 0.1ì´ˆ ê°„ê²©
                    
                    if i % 10 == 0:
                        print(f"ë¡œë”© ì§„í–‰ë¥ : {i+1}/{len(pcd_files)} ({(i+1)/len(pcd_files)*100:.1f}%)")
                        print(f"   velocity field: {'âœ…' if len(velocities) > 0 else 'âŒ'}")
                        
            except Exception as e:
                print(f"âš ï¸ {pcd_file} ë¡œë”© ì‹¤íŒ¨: {e}")
                continue
        
        print(f"âœ… {len(self.point_clouds)}ê°œì˜ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ë¡œë”© ì™„ë£Œ")
        return self.point_clouds
    
    def load_vehicle_speed_data(self, speed_file_path: str) -> List[float]:
        """
        ì°¨ëŸ‰ ì†ë„ ë°ì´í„° ë¡œë”©
        
        Args:
            speed_file_path: ì†ë„ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (CSV ë˜ëŠ” TXT)
            
        Returns:
            ì°¨ëŸ‰ ì†ë„ ë¦¬ìŠ¤íŠ¸ [m/s]
        """
        try:
            if speed_file_path.endswith('.csv'):
                speed_data = pd.read_csv(speed_file_path)
                # ì²« ë²ˆì§¸ ì—´ì„ ì†ë„ë¡œ ê°€ì •
                speeds = speed_data.iloc[:, 0].values
            else:  # txt íŒŒì¼
                speeds = np.loadtxt(speed_file_path)
            
            # í¬ì¸íŠ¸í´ë¼ìš°ë“œ ê°œìˆ˜ì™€ ë§ì¶¤
            if len(speeds) != len(self.point_clouds):
                print(f"âš ï¸ ì†ë„ ë°ì´í„° ê°œìˆ˜({len(speeds)})ì™€ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ê°œìˆ˜({len(self.point_clouds)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
                speeds = np.resize(speeds, len(self.point_clouds))
            
            self.vehicle_speeds = speeds.tolist()
            print(f"âœ… ì°¨ëŸ‰ ì†ë„ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.vehicle_speeds)}ê°œ")
            
        except Exception as e:
            print(f"âš ï¸ ì†ë„ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ëª¨ë“  ì†ë„ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            self.vehicle_speeds = [0.0] * len(self.point_clouds)
        
        return self.vehicle_speeds
    
    def _load_pcd_with_velocity(self, pcd_file: str) -> tuple:
        """
        PCD íŒŒì¼ì—ì„œ velocity fieldë¥¼ í¬í•¨í•˜ì—¬ ë¡œë”©
        
        Args:
            pcd_file: PCD íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (point_cloud, velocities) íŠœí”Œ
        """
        try:
            # 1. í—¤ë” íŒŒì‹±
            with open(pcd_file, 'r') as f:
                lines = f.readlines()
            
            header_info = {}
            data_start_idx = 0
            
            for i, line in enumerate(lines):
                line = line.strip()
                if line.startswith('FIELDS'):
                    header_info['fields'] = line.split()[1:]
                elif line.startswith('SIZE'):
                    header_info['sizes'] = [int(x) for x in line.split()[1:]]
                elif line.startswith('TYPE'):
                    header_info['types'] = line.split()[1:]
                elif line.startswith('COUNT'):
                    header_info['counts'] = [int(x) for x in line.split()[1:]]
                elif line.startswith('POINTS'):
                    header_info['points'] = int(line.split()[1])
                elif line.startswith('DATA'):
                    header_info['data_type'] = line.split()[1]
                    data_start_idx = i + 1
                    break
            
            # 2. field ì¸ë±ìŠ¤ ì°¾ê¸°
            x_idx = header_info['fields'].index('x') if 'x' in header_info['fields'] else None
            y_idx = header_info['fields'].index('y') if 'y' in header_info['fields'] else None  
            z_idx = header_info['fields'].index('z') if 'z' in header_info['fields'] else None
            vel_idx = None
            
            if 'vel' in header_info['fields']:
                vel_idx = header_info['fields'].index('vel')
            elif 'velocity' in header_info['fields']:
                vel_idx = header_info['fields'].index('velocity')
            
            if x_idx is None or y_idx is None or z_idx is None:
                raise ValueError("x, y, z í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 3. ë°ì´í„° ì½ê¸°
            points = []
            velocities = []
            
            if header_info['data_type'] == 'ascii':
                for line in lines[data_start_idx:]:
                    if line.strip():
                        values = line.strip().split()
                        if len(values) >= max(x_idx, y_idx, z_idx) + 1:
                            try:
                                x = float(values[x_idx])
                                y = float(values[y_idx])
                                z = float(values[z_idx])
                                points.append([x, y, z])
                                
                                if vel_idx is not None and len(values) > vel_idx:
                                    vel = float(values[vel_idx])
                                    velocities.append(vel)
                                else:
                                    velocities.append(0.0)  # ê¸°ë³¸ê°’
                                    
                            except (ValueError, IndexError):
                                continue
            else:
                # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ëŠ” ì¼ë‹¨ Open3Dë¡œ fallback
                print(f"âš ï¸ ë°”ì´ë„ˆë¦¬ PCDëŠ” Open3Dë¡œ fallback: {pcd_file}")
                pcd = o3d.io.read_point_cloud(pcd_file)
                points = np.asarray(pcd.points)
                velocities = [0.0] * len(points)  # velocity ì •ë³´ ì—†ìŒ
            
            # 4. Open3D PointCloud ìƒì„±
            pcd = o3d.geometry.PointCloud()
            if points:
                pcd.points = o3d.utility.Vector3dVector(np.array(points))
                
                # velocityë¥¼ ìƒ‰ìƒìœ¼ë¡œ ì‹œê°í™” (ì„ íƒì )
                if velocities:
                    vel_array = np.array(velocities)
                    # velocity ì •ê·œí™” (-20 ~ +20 m/s ë²”ìœ„)
                    normalized_vel = np.clip((vel_array + 20) / 40, 0, 1)
                    # colors = plt.cm.gist_rainbow(normalized_vel)[:, :3]  # RGBë§Œ ì‚¬ìš©
                    colors = plt.cm.brg(normalized_vel)[:, :3]  # RGBë§Œ ì‚¬ìš©
                    pcd.colors = o3d.utility.Vector3dVector(colors)
            
            return pcd, np.array(velocities) if velocities else np.array([])
            
        except Exception as e:
            print(f"âš ï¸ ì»¤ìŠ¤í…€ PCD ë¡œë”© ì‹¤íŒ¨, Open3Dë¡œ fallback: {e}")
            # Open3Dë¡œ fallback
            pcd = o3d.io.read_point_cloud(pcd_file)
            velocities = np.zeros(len(pcd.points))  # velocity ì •ë³´ ì—†ìŒ
            return pcd, velocities
    
    def _generate_simulation_radar_data(self) -> List[o3d.geometry.PointCloud]:
        """ì‹œë®¬ë ˆì´ì…˜ ë ˆì´ë” ë°ì´í„° ìƒì„±"""
        print("ğŸ”§ ì‹œë®¬ë ˆì´ì…˜ ë ˆì´ë” ë°ì´í„° ìƒì„± ì¤‘...")
        
        n_frames = 50
        point_clouds = []
        
        # ê³ ì • ê°ì²´ë“¤ (ì •ì )
        static_objects = [
            np.array([10, 5, 0]),   # ìš°ì¸¡ ì •ì  ê°ì²´
            np.array([15, -3, 0]),  # ì¢Œì¸¡ ì •ì  ê°ì²´
            np.array([25, 0, 0]),   # ì „ë°© ì •ì  ê°ì²´
        ]
        
        # ì´ë™ ê°ì²´ë“¤ (ë™ì )
        for frame_idx in range(n_frames):
            pcd = o3d.geometry.PointCloud()
            points = []
            velocities = []
            
            # ì •ì  ê°ì²´ë“¤ ì¶”ê°€
            for static_obj in static_objects:
                # ê° ê°ì²´ ì£¼ë³€ì— ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì¶”ê°€
                noise_points = static_obj + np.random.normal(0, 0.5, (5, 3))
                points.extend(noise_points)
                velocities.extend([0.0] * 5)  # ì •ì  ê°ì²´ëŠ” ì†ë„ 0
            
            # ë™ì  ê°ì²´ 1: ìš°ì¸¡ì—ì„œ ì¢Œì¸¡ìœ¼ë¡œ ì´ë™
            dynamic1_x = 20 - frame_idx * 0.3
            dynamic1_y = 8
            if dynamic1_x > -5:  # ì‹œì•¼ ë²”ìœ„ ë‚´
                dynamic1_points = np.array([dynamic1_x, dynamic1_y, 0]) + np.random.normal(0, 0.3, (3, 3))
                points.extend(dynamic1_points)
                velocities.extend([-3.0] * 3)  # -3 m/s ì†ë„
            
            # ë™ì  ê°ì²´ 2: ì¢Œì¸¡ì—ì„œ ìš°ì¸¡ìœ¼ë¡œ ì´ë™
            dynamic2_x = -5 + frame_idx * 0.4
            dynamic2_y = -6
            if dynamic2_x < 30:  # ì‹œì•¼ ë²”ìœ„ ë‚´
                dynamic2_points = np.array([dynamic2_x, dynamic2_y, 0]) + np.random.normal(0, 0.3, (4, 3))
                points.extend(dynamic2_points)
                velocities.extend([4.0] * 4)  # +4 m/s ì†ë„
            
            # ëœë¤ ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ë“¤
            noise_points = np.random.uniform([-5, -10, -1], [30, 10, 1], (10, 3))
            points.extend(noise_points)
            velocities.extend(np.random.normal(0, 1, 10))  # ëœë¤ ì†ë„
            
            # í¬ì¸íŠ¸í´ë¼ìš°ë“œ ìƒì„±
            if points:
                pcd.points = o3d.utility.Vector3dVector(np.array(points))
                
                # ì†ë„ ì •ë³´ë¥¼ ì»¬ëŸ¬ë¡œ ë§¤í•‘ (ë„í”ŒëŸ¬ ì†ë„)
                vel_array = np.array(velocities)
                # ì†ë„ ì •ê·œí™” (-5 ~ +5 m/s ë²”ìœ„)
                normalized_vel = np.clip((vel_array + 5) / 10, 0, 1)
                colors = plt.cm.RdYlBu_r(normalized_vel)[:, :3]  # RGBë§Œ ì‚¬ìš©
                pcd.colors = o3d.utility.Vector3dVector(colors)
                
                # ì†ë„ ì •ë³´ë¥¼ ì‚¬ìš©ì ì •ì˜ ì†ì„±ìœ¼ë¡œ ì €ì¥
                # Open3Dì—ì„œëŠ” ì§ì ‘ì ì¸ custom attribute ì§€ì›ì´ ì œí•œì ì´ë¯€ë¡œ
                # ë³„ë„ë¡œ ê´€ë¦¬í•˜ê±°ë‚˜ colorë¥¼ í†µí•´ ì—­ì‚°
                
            point_clouds.append(pcd)
            self.timestamps.append(frame_idx * 0.1)
        
        self.point_clouds = point_clouds
        self.vehicle_speeds = [5.0] * n_frames  # 5 m/s ì¼ì • ì†ë„
        
        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì—ì„œ velocity ì •ë³´ ìƒì„± (ìƒ‰ìƒì—ì„œ ì—­ì‚°)
        simulation_velocities = []
        for pcd in point_clouds:
            if pcd.has_colors():
                colors = np.asarray(pcd.colors)
                # RdYlBu_r ì»¬ëŸ¬ë§µì—ì„œ ì†ë„ ì—­ì‚° (-5 ~ +5 m/s ë²”ìœ„)
                velocities = (1 - colors[:, 0]) * 10 - 5
                simulation_velocities.append(velocities)
            else:
                simulation_velocities.append(np.zeros(len(pcd.points)))
        
        self.point_velocities = simulation_velocities
        
        print(f"âœ… {len(point_clouds)}ê°œì˜ ì‹œë®¬ë ˆì´ì…˜ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ")
        return point_clouds

class RadarVisualizer:
    """ë ˆì´ë” ë°ì´í„° ì‹œê°í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.vis = None

    def visualize_single_frame(self, pcd: o3d.geometry.PointCloud, frame_idx: int = 0, visualize_coordinate_frame: bool = True):
        """ë‹¨ì¼ í”„ë ˆì„ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ì‹œê°í™”"""
        print(f"ğŸ“Š í”„ë ˆì„ {frame_idx} ì‹œê°í™” ì¤‘...")
        
        # ì¢Œí‘œì¶• ì¶”ê°€
        coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=2.0)
        if visualize_coordinate_frame:
            geometries = [pcd, coordinate_frame]
        else:
            geometries = [pcd]

        # # point cloudì˜ color map ì§€ì •
        # colors = np.asarray(pcd.colors)
        # colors = plt.cm.hsv(colors)
        # pcd.colors = o3d.utility.Vector3dVector(colors)
        
        # ì‹œê°í™”
        o3d.visualization.draw_geometries(
            geometries,
            window_name=f"Radar Point Cloud - Frame {frame_idx}",
            width=1200,
            height=800,
            left=50,
            top=50
        )
    
    def visualize_accumulated_frames(self, point_clouds: List[o3d.geometry.PointCloud], 
                                   max_frames: int = None):
        """ëˆ„ì ëœ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ì‹œê°í™”"""
        if max_frames is None:
            max_frames = len(point_clouds)
        
        print(f"ğŸ“Š {min(max_frames, len(point_clouds))}ê°œ í”„ë ˆì„ ëˆ„ì  ì‹œê°í™” ì¤‘...")
        
        # ëª¨ë“  í¬ì¸íŠ¸ ëˆ„ì 
        accumulated_points = []
        accumulated_colors = []
        
        for i, pcd in enumerate(point_clouds[:max_frames]):
            if len(pcd.points) > 0:
                points = np.asarray(pcd.points)
                colors = np.asarray(pcd.colors) if pcd.has_colors() else np.ones((len(points), 3)) * 0.5
                
                # ì‹œê°„ì— ë”°ë¥¸ íˆ¬ëª…ë„ íš¨ê³¼ (ìµœì‹ ì´ ë” ì§„í•¨)
                alpha = 0.3 + 0.7 * (i / max_frames)
                colors = colors * alpha
                
                accumulated_points.append(points)
                accumulated_colors.append(colors)
        
        if accumulated_points:
            # ëˆ„ì  í¬ì¸íŠ¸í´ë¼ìš°ë“œ ìƒì„±
            all_points = np.vstack(accumulated_points)
            all_colors = np.vstack(accumulated_colors)
            
            accumulated_pcd = o3d.geometry.PointCloud()
            accumulated_pcd.points = o3d.utility.Vector3dVector(all_points)
            accumulated_pcd.colors = o3d.utility.Vector3dVector(all_colors)
            
            # ì¢Œí‘œì¶• ì¶”ê°€
            coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=3.0)
            
            # ì‹œê°í™”
            o3d.visualization.draw_geometries(
                [accumulated_pcd, coordinate_frame],
                window_name=f"Accumulated Radar Point Cloud ({max_frames} frames)",
                width=1200,
                height=800,
                left=50,
                top=50
            )
    
    def create_animation(self, point_clouds: List[o3d.geometry.PointCloud], 
                        timestamps: List[float], 
                        save_path: str = None,
                        fps: int = 10):
        """í¬ì¸íŠ¸í´ë¼ìš°ë“œ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
        print(f"ğŸ¬ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì¤‘... ({len(point_clouds)}ê°œ í”„ë ˆì„)")
        
        # Open3D ì• ë‹ˆë©”ì´ì…˜ì€ ë³µì¡í•˜ë¯€ë¡œ matplotlib ê¸°ë°˜ìœ¼ë¡œ 2D ì‹œê°í™”
        fig, ax = plt.subplots(figsize=(24, 8))
        
        def animate(frame_idx):
            ax.clear()
            
            if frame_idx < len(point_clouds):
                pcd = point_clouds[frame_idx]
                if len(pcd.points) > 0:
                    points = np.asarray(pcd.points)
                    colors = np.asarray(pcd.colors) if pcd.has_colors() else 'blue'
                    
                    # XY í‰ë©´ íˆ¬ì˜
                    ax.scatter(points[:, 0], points[:, 1], c=colors, s=20, alpha=0.7)
            
            ax.set_xlim(-10, 80)
            ax.set_ylim(-15, 15)
            ax.set_xlabel('X [m]')
            ax.set_ylabel('Y [m]')
            ax.set_title(f'Radar Point Cloud Animation - Frame {frame_idx} (t={timestamps[frame_idx]:.1f}s)')
            ax.grid(True, alpha=0.3)
            ax.patch.set_facecolor('black')
            ax.set_aspect('equal')
        
        # ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        anim = FuncAnimation(fig, animate, frames=len(point_clouds), 
                           interval=1000//fps, repeat=True)
        
        if save_path:
            print(f"ğŸ’¾ ì• ë‹ˆë©”ì´ì…˜ì„ {save_path}ì— ì €ì¥ ì¤‘...")
            if save_path.endswith('.gif'):
                writer = PillowWriter(fps=fps)
                anim.save(save_path, writer=writer)
            else:
                anim.save(save_path, fps=fps)
            print("âœ… ì• ë‹ˆë©”ì´ì…˜ ì €ì¥ ì™„ë£Œ!")
        else:
            plt.show()
        
        return anim

###############################################################################
# 2. Noise Filtering
class StatisticalOutlierRemoval:
    """
    Statistical Outlier Removal filter for point cloud noise filtering using custom KD-Tree.
    
    This class implements the statistical outlier removal algorithm that identifies
    and removes points based on their statistical distribution of distances to neighbors.
    
    Theoretical Background:
    ----------------------
    1. For each point P, find its k nearest neighbors using KD-Tree
    2. Compute the mean distance from P to its k neighbors
    3. Calculate global statistics (mean Î¼, standard deviation Ïƒ) of all mean distances
    4. Set thresholds: lower = Î¼ - (std_ratio Ã— Ïƒ), upper = Î¼ + (std_ratio Ã— Ïƒ)
    5. Points with mean distances outside [lower, upper] are considered outliers
    
    The assumption is that in a clean point cloud, most points should have
    similar mean distances to their neighbors. Outliers will have significantly
    different (usually larger) mean distances.
    """
    
    def __init__(self, nb_neighbors: int = 20, std_ratio: float = 2.0):
        """
        Initialize the Statistical Outlier Removal filter.
        
        Args:
            nb_neighbors (int): Number of nearest neighbors to consider for each point
                              - Larger values: more robust but slower
                              - Smaller values: faster but less robust
            std_ratio (float): Standard deviation ratio threshold for outlier detection
                             - Larger values: more tolerant (fewer outliers detected)
                             - Smaller values: more strict (more outliers detected)
        """
        self.nb_neighbors = nb_neighbors
        self.std_ratio = std_ratio
        
        # Results from analysis (will be set after filtering)
        self.kdtree_root_ = None
        self.mean_distances_ = None
        self.global_mean_ = None
        self.global_std_ = None
        self.threshold_lower_ = None
        self.threshold_upper_ = None
        self.processing_stats_ = {}
        
    def compute_mean_distances(self, points: np.ndarray) -> np.ndarray:
        """
        Compute the mean distance from each point to its k nearest neighbors using custom KD-Tree.
        
        This method follows the theoretical procedure:
        1. Build a KD-Tree from all points for efficient neighbor search
        2. For each point, find its k nearest neighbors using the KD-Tree
        3. Calculate the mean distance from each point to its neighbors
        4. Return array of mean distances for all points
        
        Args:
            points (np.ndarray): Input point cloud of shape (N, D)
            
        Returns:
            np.ndarray: Mean distances for each point of shape (N,)
        """
        print(f"Computing mean distances for {len(points)} points using custom KD-Tree...")
        print(f"Building KD-Tree structure...")
        
        # Step 1: Build KD-Tree from all points
        kdtree_build_start = time.time()
        # Create point cloud object for KD-Tree
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        self.kdtree_root_ = o3d.geometry.KDTreeFlann(pcd)
        kdtree_build_time = time.time() - kdtree_build_start
        
        if self.kdtree_root_ is None:
            raise ValueError("Failed to build KD-Tree from input points")
        
        print(f"KD-Tree built successfully in {kdtree_build_time:.4f}s. Starting k-NN search for each point...")
        
        # Store processing statistics
        self.processing_stats_['kdtree_build_time'] = kdtree_build_time
        self.processing_stats_['total_points'] = len(points)
        self.processing_stats_['point_dimension'] = points.shape[1]
        
        # Step 2: For each point, find k nearest neighbors and compute mean distance
        mean_distances = np.zeros(len(points))
        knn_search_start = time.time()
        total_searches = 0
        
        for i, query_point in enumerate(points):
            if i % 100 == 0 and i > 0:
                print(f"  Processed {i}/{len(points)} points ({i/len(points)*100:.1f}%)")
            
            # Step 2a: Find k+1 nearest neighbors (including the point itself)
            [_, neighbor_indices, squared_distances] = self.kdtree_root_.search_knn_vector_3d(xxxxxx, xxxxxx) # TODO: k-NN ê²€ìƒ‰ í•¨ìˆ˜ì˜ ì¸ìë¥¼ ì±„ì›Œì£¼ì„¸ìš”.
            total_searches += 1
            
            # Step 2b: Remove the first neighbor (the point itself with distance 0)
            # The first neighbor should be the point itself with very small distance (due to floating point precision)
            if len(neighbor_indices) > 1:
                # Skip the first neighbor (self) and take the next k neighbors
                actual_neighbor_distances = xxxxxx # TODO: squared_distancesì—ì„œ ìì‹ ê³¼ì˜ ê±°ë¦¬ë¥¼ ì œì™¸í•´ì£¼ì„¸ìš”.
                
                # Convert squared distances to actual distances
                neighbor_distances = np.sqrt(actual_neighbor_distances)
                
                # Step 2c: Compute mean distance to k neighbors
                mean_distances[i] = np.mean(neighbor_distances)
            else:
                # If only one neighbor found (shouldn't happen in normal cases)
                mean_distances[i] = 0.0
                print(f"Warning: Only found {len(neighbor_indices)} neighbor(s) for point {i}")
        
        knn_search_time = time.time() - knn_search_start
        
        print(f"Mean distance computation completed in {knn_search_time:.4f}s.")
        print(f"Average time per k-NN search: {knn_search_time/total_searches*1000:.3f}ms")
        print(f"Distance range: [{np.min(mean_distances):.4f}, {np.max(mean_distances):.4f}]")
        
        # Store more processing statistics
        self.processing_stats_['knn_search_time'] = knn_search_time
        self.processing_stats_['total_knn_searches'] = total_searches
        self.processing_stats_['avg_search_time_ms'] = knn_search_time/total_searches*1000
        
        return mean_distances
    
    def compute_statistics(self, mean_distances: np.ndarray) -> Tuple[float, float, float, float]:
        """
        Compute global statistics of mean distances.
        
        Args:
            mean_distances (np.ndarray): Mean distances for each point
            
        Returns:
            Tuple[float, float, float, float]: global_mean, global_std, threshold_lower, threshold_upper
        """
        global_mean = np.mean(mean_distances)
        global_std = np.std(mean_distances)
        
        threshold_lower = xxxxxx # TODO: self.std_ratioë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜í•œ ì„ê³„ê°’ ê³„ì‚°
        threshold_upper = xxxxxx # TODO: self.std_ratioë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒí•œ ì„ê³„ê°’ ê³„ì‚°
        
        return global_mean, global_std, threshold_lower, threshold_upper
    
    def find_outliers(self, mean_distances: np.ndarray, 
                     threshold_lower: float, threshold_upper: float) -> np.ndarray:
        """
        Find outlier indices based on statistical thresholds.
        
        Args:
            mean_distances (np.ndarray): Mean distances for each point
            threshold_lower (float): Lower threshold for outlier detection
            threshold_upper (float): Upper threshold for outlier detection
            
        Returns:
            np.ndarray: Boolean mask where True indicates outliers
        """
        outlier_mask = (mean_distances < threshold_lower) | (mean_distances > threshold_upper)
        return outlier_mask
    
    def filter_points_sor(self, points: np.ndarray, return_outliers: bool = False) -> Tuple[np.ndarray, np.ndarray]:
        """
        Apply statistical outlier removal to the input point cloud.
        
        Args:
            points (np.ndarray): Input point cloud of shape (N, D)
            return_outliers (bool): If True, also return the outlier points
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: (filtered_points, outlier_indices or outlier_points)
        """
        start_time = time.time()
        
        print(f"Applying Statistical Outlier Removal...")
        print(f"Parameters: nb_neighbors={self.nb_neighbors}, std_ratio={self.std_ratio}")
        print(f"Input points: {len(points)}")
        
        # Step 1: Compute mean distances
        self.mean_distances_ = self.compute_mean_distances(points) # TODO: ì»¤ìŠ¤í…€ Statistical Outlier Removal í´ë˜ìŠ¤ì˜ compute_mean_distances í•¨ìˆ˜ ì™„ì„±
        
        # Step 2: Compute global statistics
        self.global_mean_, self.global_std_, self.threshold_lower_, self.threshold_upper_ = \
            self.compute_statistics(self.mean_distances_) # TODO: ì»¤ìŠ¤í…€ Statistical Outlier Removal í´ë˜ìŠ¤ì˜ compute_statistics í•¨ìˆ˜ ì™„ì„±
        
        print(f"Global mean distance: {self.global_mean_:.4f}")
        print(f"Global std distance: {self.global_std_:.4f}")
        print(f"Threshold range: [{self.threshold_lower_:.4f}, {self.threshold_upper_:.4f}]")
        
        # Step 3: Find outliers
        outlier_mask = self.find_outliers(self.mean_distances_, 
                                         self.threshold_lower_, self.threshold_upper_)
        
        # Step 4: Filter points
        inlier_mask = ~outlier_mask
        filtered_points = xxxxxx # TODO: inlier_maskë¥¼ ì‚¬ìš©í•˜ì—¬ í•„í„°ë§ëœ í¬ì¸íŠ¸ë¥¼ ì„ íƒ
        
        num_outliers = np.sum(outlier_mask)
        num_inliers = len(filtered_points)
        
        print(f"Outliers detected: {num_outliers}")
        print(f"Inliers remaining: {num_inliers}")
        print(f"Outlier ratio: {num_outliers/len(points)*100:.2f}%")
        print(f"Processing time: {time.time() - start_time:.4f} seconds")
        
        if return_outliers:
            outlier_points = points[outlier_mask]
            return filtered_points, outlier_points
        else:
            return filtered_points, np.where(outlier_mask)[0]
    
    def analyze_point_neighborhoods(self, points: np.ndarray, sample_points: List[int] = None) -> Dict[str, Any]:
        """
        Analyze the neighborhood structure of specific points for educational purposes.
        
        Args:
            points (np.ndarray): Original point cloud
            sample_points (List[int]): Indices of points to analyze. If None, analyze random sample.
            
        Returns:
            Dict[str, Any]: Analysis results with neighborhood information
        """
        if self.kdtree_root_ is None:
            raise ValueError("Must run filter_points_sor() first to build KD-Tree")
        
        if sample_points is None:
            # Select 5 random points for analysis
            sample_points = np.random.choice(len(points), size=min(5, len(points)), replace=False)
        
        analysis = {
            'sample_points': sample_points,
            'neighborhood_analysis': []
        }
        
        print("=== Neighborhood Analysis ===")
        for i, point_idx in enumerate(sample_points):
            query_point = points[point_idx]
            
            # Find neighbors
            [_, neighbor_indices, squared_distances] = self.kdtree_root_.search_knn_vector_3d(query_point, self.nb_neighbors + 1)
            
            # Remove self and convert to actual distances
            neighbor_indices = neighbor_indices[1:]
            distances = np.sqrt(squared_distances[1:])
            mean_dist = np.mean(distances)
            
            point_analysis = {
                'point_index': int(point_idx),
                'coordinates': query_point.tolist(),
                'neighbor_indices': [int(idx) for idx in neighbor_indices],
                'distances': distances.tolist(),
                'mean_distance': float(mean_dist),
                'min_distance': float(np.min(distances)),
                'max_distance': float(np.max(distances)),
                'std_distance': float(np.std(distances))
            }
            
            if self.mean_distances_ is not None:
                stored_mean = self.mean_distances_[point_idx]
                point_analysis['stored_mean_distance'] = float(stored_mean)
                point_analysis['is_outlier'] = self._is_point_outlier(stored_mean)
            
            analysis['neighborhood_analysis'].append(point_analysis)
            
            print(f"\nPoint {point_idx} at {query_point}:")
            print(f"  Neighbors: {neighbor_indices[:5]}{'...' if len(neighbor_indices) > 5 else ''}")
            print(f"  Mean distance: {mean_dist:.4f}")
            print(f"  Distance range: [{np.min(distances):.4f}, {np.max(distances):.4f}]")
            if self.mean_distances_ is not None:
                print(f"  Outlier status: {'OUTLIER' if self._is_point_outlier(stored_mean) else 'INLIER'}")
        
        return analysis
    
    def _is_point_outlier(self, mean_distance: float) -> bool:
        """Check if a point is an outlier based on its mean distance."""
        if self.threshold_lower_ is None or self.threshold_upper_ is None:
            return False
        return mean_distance < self.threshold_lower_ or mean_distance > self.threshold_upper_
    
    def get_processing_summary(self) -> Dict[str, Any]:
        """
        Get a summary of the processing statistics for educational analysis.
        
        Returns:
            Dict[str, Any]: Processing statistics and algorithm performance metrics
        """
        if not self.processing_stats_:
            return {"error": "No processing statistics available. Run filter_points_sor() first."}
        
        summary = {
            'algorithm_parameters': {
                'nb_neighbors': self.nb_neighbors,
                'std_ratio': self.std_ratio
            },
            'data_statistics': {
                'total_points': self.processing_stats_.get('total_points', 0),
                'point_dimension': self.processing_stats_.get('point_dimension', 0),
            },
            'performance_metrics': {
                'kdtree_build_time_s': self.processing_stats_.get('kdtree_build_time', 0),
                'knn_search_time_s': self.processing_stats_.get('knn_search_time', 0),
                'total_knn_searches': self.processing_stats_.get('total_knn_searches', 0),
                'avg_search_time_ms': self.processing_stats_.get('avg_search_time_ms', 0),
            },
            'filtering_results': {}
        }
        
        if self.mean_distances_ is not None:
            summary['filtering_results'].update({
                'global_mean_distance': float(self.global_mean_),
                'global_std_distance': float(self.global_std_),
                'threshold_lower': float(self.threshold_lower_),
                'threshold_upper': float(self.threshold_upper_),
                'distance_range': [float(np.min(self.mean_distances_)), float(np.max(self.mean_distances_))]
            })
        
        return summary
    
    def explain_algorithm_steps(self) -> None:
        """
        Print a detailed explanation of the Statistical Outlier Removal algorithm steps.
        """
        print("=" * 60)
        print("Statistical Outlier Removal Algorithm - Step by Step")
        print("=" * 60)
        print(
            """
THEORETICAL BACKGROUND:
The algorithm assumes that in a clean point cloud, most points should have
similar mean distances to their k nearest neighbors. Outlier points will
typically have significantly different (usually larger) mean distances.

ALGORITHM STEPS:

Step 1: KD-Tree Construction
   - Build a KD-Tree from all input points for efficient neighbor search
   - Time complexity: O(n log n) where n is the number of points
   - Space complexity: O(n)

Step 2: Neighborhood Analysis
   For each point P in the point cloud:
   a) Use KD-Tree to find k nearest neighbors of P
   b) Calculate distances from P to each of its k neighbors
   c) Compute mean distance: Î¼_P = (1/k) * Î£(distance(P, neighbor_i))

Step 3: Statistical Analysis
   - Collect all mean distances: [Î¼_P1, Î¼_P2, ..., Î¼_Pn]
   - Calculate global statistics:
     * Global mean: Î¼ = (1/n) * Î£(Î¼_Pi)
     * Global standard deviation: Ïƒ = sqrt((1/n) * Î£(Î¼_Pi - Î¼)Â²)

Step 4: Threshold Calculation
   - Lower threshold: threshold_lower = Î¼ - (std_ratio * Ïƒ)
   - Upper threshold: threshold_upper = Î¼ + (std_ratio * Ïƒ)

Step 5: Outlier Classification
   A point P is classified as an outlier if:
   Î¼_P < threshold_lower OR Î¼_P > threshold_upper

PARAMETERS:
- nb_neighbors (k): Number of neighbors to consider
  * Larger k: More robust, slower computation
  * Smaller k: Faster, but may be less reliable
  
- std_ratio: Controls sensitivity of outlier detection
  * Larger std_ratio: More tolerant (fewer outliers detected)
  * Smaller std_ratio: More strict (more outliers detected)
  * Typical values: 1.0 (strict) to 3.0 (tolerant)

COMPUTATIONAL COMPLEXITY:
- KD-Tree construction: O(n log n)
- k-NN search for all points: O(n log n) average case
- Overall: O(n log n)
            """
        )
        print("=" * 60)


def statistical_outlier_removal(points: np.ndarray, 
                               nb_neighbors: int = 20, 
                               std_ratio: float = 2.0) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convenience function for statistical outlier removal.
    
    Args:
        points (np.ndarray): Input point cloud of shape (N, D)
        nb_neighbors (int): Number of nearest neighbors to consider
        std_ratio (float): Standard deviation ratio threshold
        
    Returns:
        Tuple[np.ndarray, np.ndarray]: (filtered_points, outlier_indices)
    """
    filter_obj = StatisticalOutlierRemoval(nb_neighbors=nb_neighbors, std_ratio=std_ratio)
    return filter_obj.filter_points_sor(points)


def visualize_outlier_removal_2d(original_points: np.ndarray, 
                                filtered_points: np.ndarray, 
                                outlier_points: np.ndarray,
                                title: str = "Statistical Outlier Removal Results") -> None:
    """
    Visualize the results of statistical outlier removal for 2D point clouds.
    
    Args:
        original_points (np.ndarray): Original point cloud
        filtered_points (np.ndarray): Points after filtering
        outlier_points (np.ndarray): Detected outlier points
        title (str): Plot title
    """
    plt.figure(figsize=(15, 5))
    
    # Original points
    plt.subplot(1, 3, 1)
    plt.scatter(original_points[:, 0], original_points[:, 1], c='gray', s=20, alpha=0.6)
    plt.title(f'Original Points\n({len(original_points)} points)')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    
    # Filtered points vs outliers
    plt.subplot(1, 3, 2)
    plt.scatter(filtered_points[:, 0], filtered_points[:, 1], c='blue', s=20, alpha=0.7, label='Inliers')
    if len(outlier_points) > 0:
        plt.scatter(outlier_points[:, 0], outlier_points[:, 1], c='red', s=30, alpha=0.8, label='Outliers')
    plt.title(f'Filtering Results\nInliers: {len(filtered_points)}, Outliers: {len(outlier_points)}')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    
    # Filtered points only
    plt.subplot(1, 3, 3)
    plt.scatter(filtered_points[:, 0], filtered_points[:, 1], c='green', s=20, alpha=0.7)
    plt.title(f'Filtered Points\n({len(filtered_points)} points)')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()


def visualize_distance_distribution(mean_distances: np.ndarray, 
                                  global_mean: float, 
                                  global_std: float,
                                  threshold_lower: float, 
                                  threshold_upper: float,
                                  outlier_mask: np.ndarray) -> None:
    """
    Visualize the distribution of mean distances and outlier thresholds.
    
    Args:
        mean_distances (np.ndarray): Mean distances for each point
        global_mean (float): Global mean distance
        global_std (float): Global standard deviation
        threshold_lower (float): Lower threshold
        threshold_upper (float): Upper threshold
        outlier_mask (np.ndarray): Boolean mask for outliers
    """
    plt.figure(figsize=(12, 5))
    
    # Distance distribution histogram
    plt.subplot(1, 2, 1)
    plt.hist(mean_distances, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(global_mean, color='blue', linestyle='--', linewidth=2, label=f'Mean: {global_mean:.4f}')
    plt.axvline(threshold_lower, color='red', linestyle='--', linewidth=2, label=f'Lower: {threshold_lower:.4f}')
    plt.axvline(threshold_upper, color='red', linestyle='--', linewidth=2, label=f'Upper: {threshold_upper:.4f}')
    plt.xlabel('Mean Distance to Neighbors')
    plt.ylabel('Frequency')
    plt.title('Distribution of Mean Distances')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Outlier vs inlier distances
    plt.subplot(1, 2, 2)
    inlier_distances = mean_distances[~outlier_mask]
    outlier_distances = mean_distances[outlier_mask]
    
    plt.hist(inlier_distances, bins=30, alpha=0.7, color='green', label=f'Inliers ({len(inlier_distances)})')
    if len(outlier_distances) > 0:
        plt.hist(outlier_distances, bins=20, alpha=0.7, color='red', label=f'Outliers ({len(outlier_distances)})')
    
    plt.xlabel('Mean Distance to Neighbors')
    plt.ylabel('Frequency')
    plt.title('Inliers vs Outliers Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def create_noisy_point_cloud_2d(n_points: int = 1000, outlier_ratio: float = 0.1) -> np.ndarray:
    """
    Create a 2D point cloud with noise for testing purposes.
    
    Args:
        n_points (int): Total number of points
        outlier_ratio (float): Ratio of noise points (0.0 to 1.0)
        
    Returns:
        np.ndarray: 2D point cloud with noise
    """
    np.random.seed(42)
    
    n_clean = int(n_points * (1 - outlier_ratio))
    n_noise = n_points - n_clean
    
    # Create clean points (clustered)
    center1 = np.random.multivariate_normal([2, 2], [[0.5, 0], [0, 0.5]], n_clean // 2)
    center2 = np.random.multivariate_normal([5, 5], [[0.3, 0], [0, 0.3]], n_clean // 2)
    clean_points = np.vstack([center1, center2])
    
    # Create noise points (random)
    noise_points = np.random.uniform(-1, 8, (n_noise, 2))
    
    # Combine
    all_points = np.vstack([clean_points, noise_points])
    
    # Shuffle
    indices = np.random.permutation(len(all_points))
    return all_points[indices]


# Example usage and testing
if __name__ == "__main__":
    print("=== Custom Statistical Outlier Removal Demo (Using Custom KD-Tree) ===")
    
    # Step 1: Explain the algorithm
    print("\n" + "="*60)
    print("STEP 1: Understanding the Algorithm")
    print("="*60)
    filter_obj = StatisticalOutlierRemoval(nb_neighbors=10, std_ratio=2.0)
    filter_obj.explain_algorithm_steps()
    
    # Step 2: Create test data
    print("\n" + "="*60)
    print("STEP 2: Creating Test Data")
    print("="*60)
    points_2d = create_noisy_point_cloud_2d(n_points=200, outlier_ratio=0.15)
    print(f"Created {len(points_2d)} points with ~15% noise")
    print(f"Point cloud dimensions: {points_2d.shape}")
    
    # Step 3: Apply statistical outlier removal with detailed logging
    print("\n" + "="*60)
    print("STEP 3: Applying Statistical Outlier Removal")
    print("="*60)
    print(f"Algorithm parameters:")
    print(f"  - Number of neighbors (k): {filter_obj.nb_neighbors}")
    print(f"  - Standard deviation ratio: {filter_obj.std_ratio}")
    print()
    
    filtered_points, outlier_points = filter_obj.filter_points_sor(points_2d, return_outliers=True)
    
    # Step 4: Get processing summary
    print("\n" + "="*60)
    print("STEP 4: Processing Summary")
    print("="*60)
    summary = filter_obj.get_processing_summary()
    
    print("Algorithm Performance:")
    perf = summary['performance_metrics']
    print(f"  - KD-Tree build time: {perf['kdtree_build_time_s']:.4f}s")
    print(f"  - Total k-NN search time: {perf['knn_search_time_s']:.4f}s")
    print(f"  - Average search time per point: {perf['avg_search_time_ms']:.3f}ms")
    print(f"  - Total k-NN searches performed: {perf['total_knn_searches']}")
    
    if summary['filtering_results']:
        print("\nFiltering Results:")
        results = summary['filtering_results']
        print(f"  - Global mean distance: {results['global_mean_distance']:.4f}")
        print(f"  - Global std distance: {results['global_std_distance']:.4f}")
        print(f"  - Threshold range: [{results['threshold_lower']:.4f}, {results['threshold_upper']:.4f}]")
        print(f"  - Distance range: [{results['distance_range'][0]:.4f}, {results['distance_range'][1]:.4f}]")
    
    # Step 5: Analyze specific point neighborhoods
    print("\n" + "="*60)
    print("STEP 5: Neighborhood Analysis")
    print("="*60)
    
    # Choose specific points for analysis (some inliers and potential outliers)
    outlier_indices = np.where(filter_obj.find_outliers(filter_obj.mean_distances_, 
                                                       filter_obj.threshold_lower_, 
                                                       filter_obj.threshold_upper_))[0]
    inlier_indices = np.where(~filter_obj.find_outliers(filter_obj.mean_distances_, 
                                                       filter_obj.threshold_lower_, 
                                                       filter_obj.threshold_upper_))[0]
    
    # Select 3 inliers and 2 outliers for detailed analysis
    sample_points = []
    if len(inlier_indices) >= 3:
        sample_points.extend(np.random.choice(inlier_indices, 3, replace=False))
    if len(outlier_indices) >= 2:
        sample_points.extend(np.random.choice(outlier_indices, 2, replace=False))
    
    analysis = filter_obj.analyze_point_neighborhoods(points_2d, sample_points)
    
    # Step 6: Visualize results
    print("\n" + "="*60)
    print("STEP 6: Visualization")
    print("="*60)
    print("Generating visualizations...")
    
    visualize_outlier_removal_2d(points_2d, filtered_points, outlier_points, 
                                title="Statistical Outlier Removal using Custom KD-Tree")
    
    # Visualize distance distribution
    outlier_mask = filter_obj.find_outliers(filter_obj.mean_distances_, 
                                           filter_obj.threshold_lower_, 
                                           filter_obj.threshold_upper_)
    
    visualize_distance_distribution(filter_obj.mean_distances_, 
                                  filter_obj.global_mean_, 
                                  filter_obj.global_std_,
                                  filter_obj.threshold_lower_, 
                                  filter_obj.threshold_upper_,
                                  outlier_mask)
    
    # Step 7: Summary
    print("\n" + "="*60)
    print("STEP 7: Final Summary")
    print("="*60)
    print(f"Original points: {len(points_2d)}")
    print(f"Filtered points: {len(filtered_points)}")
    print(f"Outliers removed: {len(outlier_points)}")
    print(f"Outlier ratio: {len(outlier_points)/len(points_2d)*100:.2f}%")
    print(f"Data quality improvement: {len(filtered_points)/len(points_2d)*100:.2f}% of data retained")
    
    print("\nKey insights:")
    print("- Custom KD-Tree provides efficient neighbor search")
    print("- Statistical analysis identifies points with unusual neighborhood distances")
    print("- Threshold-based classification separates clean data from noise")
    print("- Educational implementation shows each algorithmic step clearly")
    
    print("\n=== Demo completed successfully! ===")
    print("The custom implementation using KD-Tree demonstrates the theoretical")
    print("principles of Statistical Outlier Removal in an educational context.")

###############################################################################
# 3. Translation
def custom_translation(pcd: o3d.geometry.PointCloud, vector: np.ndarray) -> o3d.geometry.PointCloud:
    """translate xyz coordinates.

    Args:
        pcd: PointCloud object
        vector: (3, 1)

    Return:
        pcd_translation: PointCloud object
    """
    xyz = np.asarray(pcd.points)
    translated_xyz = xxxxxx # TODO: np.newaxisë¥¼ ì‚¬ìš©í•˜ì—¬ vectorë¥¼ xyzì— ë”í•˜ê¸°

    pcd_translation = o3d.geometry.PointCloud()
    pcd_translation.points = o3d.utility.Vector3dVector(translated_xyz)

    return pcd_translation

###############################################################################
# 4. Rotation
def custom_get_rotation_matrix(angle: np.ndarray) -> np.ndarray:
    """Get rotation matrix from angle.
    
    Args:
        angle: [roll, pitch, yaw] (degree)
    Return:
        rotation_matrix: (3, 3)
    """
    roll = np.deg2rad(angle[0])
    pitch = np.deg2rad(angle[1])
    yaw = np.deg2rad(angle[2])
    
    rotation_matrix_roll = np.array(
        [
            xxxxxx
        ]
    ) # TODO: roll íšŒì „ í–‰ë ¬ êµ¬í˜„
    rotation_matrix_pitch = np.array(
        [
            xxxxxx
        ]
    ) # TODO: pitch íšŒì „ í–‰ë ¬ êµ¬í˜„
    rotation_matrix_yaw = np.array(
        [
            xxxxxx
        ]
    ) # TODO: yaw íšŒì „ í–‰ë ¬ êµ¬í˜„
    
    rotation_matrix = xxxxxx # TODO: roll, pitch, yaw íšŒì „ í–‰ë ¬ì„ ê³±í•˜ì—¬ ìµœì¢… íšŒì „ í–‰ë ¬ êµ¬í˜„ (í–‰ë ¬ ê³±: @ ì—°ì‚°ì ì‚¬ìš©)
    
    return rotation_matrix

def custom_rotation(pcd: o3d.geometry.PointCloud, rotation_matrix: np.ndarray, center: np.ndarray = np.array([0, 0, 0])) -> o3d.geometry.PointCloud:
    """Rotate xyz.

    Args:
        pcd: PointCloud object
        rotation_matrix: (3, 3)
        center: (3)
    Return:
        pcd_rotation: PointCloud object
    """
    xyz = np.asarray(pcd.points)
    center = np.asarray(center)

    rotation_xyz = xxxxxx # TODO: íšŒì „ í–‰ë ¬ê³¼ center ì¢Œí‘œë¥¼ ì ìš©í•˜ì—¬ xyz ì¢Œí‘œ íšŒì „ (np.asarray(pcd.points).shape í™•ì¸ í•„ìˆ˜)
    rotation_xyz = xxxxxx # TODO: íšŒì „ í›„ center ì¢Œí‘œë¥¼ ë‹¤ì‹œ ë”í•˜ì—¬ xyz ì¢Œí‘œ ì›ìœ„ì¹˜

    pcd_rotation = o3d.geometry.PointCloud()
    pcd_rotation.points = o3d.utility.Vector3dVector(rotation_xyz)

    return pcd_rotation

###############################################################################
# 5. Transformation matrix
def custom_get_transformation_matrix(translation_vector: np.ndarray, rotation_matrix: np.ndarray) -> np.ndarray:
    """Get transformation matrix from translation vector, rotation matrix.
    
    Args:
        translation_vector: (3)
        rotation_matrix: (3, 3)
    Return:
        transformation_matrix: (4, 4)
    """
    transformation_matrix = np.eye(4)
    transformation_matrix[xxxxxx, xxxxxx] = rotation_matrix    # TODO: rotation_matrixì„ transformation_matrixì— ì‚½ì…
    transformation_matrix[xxxxxx, xxxxxx] = translation_vector # TODO: translation_vectorë¥¼ transformation_matrixì— ì‚½ì…

    return transformation_matrix

def custom_pcd_transformation(pcd: o3d.geometry.PointCloud, transformation_matrix: np.ndarray) -> o3d.geometry.PointCloud:
    """Transform xyz.

    Args:
        pcd: PointCloud object
        transformation_matrix: (4, 4)
    Return:
        pcd_transformation: PointCloud object
    """
    xyz = np.asarray(pcd.points)
    homogeneous_xyz = xxxxxx # TODO: (N, 4) í˜•íƒœë¡œ xyz ì¢Œí‘œì— 1ì„ ì¶”ê°€í•˜ì—¬ ë™ì°¨ ì¢Œí‘œë¡œ ë³€í™˜
    transformation_xyz = xxxxxx # TODO: transformation_matrixì™€ homogeneous_xyzë¥¼ ê³±í•˜ì—¬ ë³€í™˜ëœ ì¢Œí‘œ ê³„ì‚° (np.matmul ì‚¬ìš©)
    transformation_xyz = transformation_xyz[:, :3] # í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ë‹¤ì‹œ (N, 3) í˜•íƒœë¡œ ë³€í™˜

    pcd_transformation = o3d.geometry.PointCloud()
    pcd_transformation.points = o3d.utility.Vector3dVector(transformation_xyz)

    return pcd_transformation


###############################################################################
# 6. DBSCAN í´ëŸ¬ìŠ¤í„°ë§ êµ¬í˜„
def custom_dbscan_clustering(pcd: o3d.geometry.PointCloud, 
                            epsilon: float, 
                            min_points: int) -> Tuple[np.ndarray, int]:
    """DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    
    Args:
        pcd: í¬ì¸íŠ¸ í´ë¼ìš°ë“œ
        epsilon: ì´ì›ƒ ë°˜ì§€ë¦„
        min_points: ì½”ì–´ í¬ì¸íŠ¸ê°€ ë˜ê¸° ìœ„í•œ ìµœì†Œ ì´ì›ƒ ê°œìˆ˜
    
    Returns:
        labels: í´ëŸ¬ìŠ¤í„° ë¼ë²¨ë“¤ (-1ì€ ë…¸ì´ì¦ˆ)
        n_clusters: ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    """
    points = np.array(pcd.points)
    n_points = len(points)

    # KD-Tree ë¯¸ë¦¬ êµ¬ì¶• (í•œ ë²ˆë§Œ)
    kdtree = o3d.geometry.KDTreeFlann(pcd)
    
    # ë¼ë²¨ ì´ˆê¸°í™” (-1: ë¯¸ë¶„ë¥˜, 0~: í´ëŸ¬ìŠ¤í„° ID)
    labels = xxxxxx # TODO: np.fullì„ ì‚¬ìš©í•˜ì—¬ -1ë¡œ ì´ˆê¸°í™”ëœ ë¼ë²¨ ë°°ì—´ ìƒì„±
    cluster_id = 0
    
    for point_idx in range(n_points):
        # ì´ë¯¸ ë¶„ë¥˜ëœ í¬ì¸íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°
        if labels[point_idx] != -1:
            continue
        
        # ì´ì›ƒ í¬ì¸íŠ¸ë“¤ ì°¾ê¸°
        [num_neighbors, neighbors, distances] = xxxxxx # TODO: kdtreeì˜ search_radius_vector_3d í•¨ìˆ˜ ë° í˜„ì¬ í¬ì¸íŠ¸ ìœ„ì¹˜ì™€ epsilonì„ ì‚¬ìš©í•˜ì—¬ ì´ì›ƒ í¬ì¸íŠ¸ ì°¾ê¸°
        
        # ì½”ì–´ í¬ì¸íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° (ì´ì›ƒì´ min_pointsë³´ë‹¤ ì ìŒ)
        if xxxxxx < min_points: # TODO: ì´ì›ƒ ê°œìˆ˜ê°€ min_pointsë³´ë‹¤ ì ì€ì§€ í™•ì¸
            # ë…¸ì´ì¦ˆë¡œ ë¶„ë¥˜ (ë¼ë²¨ -1 ìœ ì§€)
            continue
        
        # ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„° ì‹œì‘
        labels[point_idx] = xxxxxx # TODO: í˜„ì¬ í´ëŸ¬ìŠ¤í„° IDë¡œ ë¼ë²¨ ì—…ë°ì´íŠ¸
        
        # ì´ì›ƒë“¤ì„ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€ (ë„ˆë¹„ ìš°ì„  íƒìƒ‰)
        seed_set = list(neighbors)
        i = 0
        while i < len(seed_set):
            current_point = xxxxxx # TODO: í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í¬ì¸íŠ¸
            
            # ë…¸ì´ì¦ˆì˜€ë˜ í¬ì¸íŠ¸ë¥¼ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€
            if labels[current_point] == -1:
                labels[current_point] = xxxxxx # TODO: í˜„ì¬ í´ëŸ¬ìŠ¤í„° IDë¡œ ë¼ë²¨ ì—…ë°ì´íŠ¸
                
                # í˜„ì¬ í¬ì¸íŠ¸ë„ ì½”ì–´ í¬ì¸íŠ¸ì¸ì§€ í™•ì¸
                [num_neighbors, current_neighbors, distances] = xxxxxx # TODO: kdtreeì˜ search_radius_vector_3d í•¨ìˆ˜ ë° í˜„ì¬ í¬ì¸íŠ¸ ìœ„ì¹˜ì™€ epsilonì„ ì‚¬ìš©í•˜ì—¬ ì´ì›ƒ í¬ì¸íŠ¸ ì°¾ê¸°
                if xxxxxx >= min_points: # TODO: ì´ì›ƒ ê°œìˆ˜ê°€ min_points ì´ìƒì¸ì§€ í™•ì¸
                    # ìƒˆë¡œìš´ ì´ì›ƒë“¤ì„ seed_setì— ì¶”ê°€
                    for neighbor in current_neighbors:
                        if neighbor not in seed_set:
                            seed_set.append(xxxxxx) # TODO: seed_setì— ìƒˆë¡œìš´ ì´ì›ƒ ì¶”ê°€
            
            i += 1
        
        cluster_id += 1
    
    return labels, cluster_id

# í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ í‰ê°€
def custom_clustering_metrics(true_labels: np.ndarray, 
                             pred_labels: np.ndarray) -> Dict[str, float]:
    """í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ í‰ê°€ ì§€í‘œë“¤ ê³„ì‚°
    
    Args:
        true_labels: ì‹¤ì œ ë¼ë²¨ë“¤
        pred_labels: ì˜ˆì¸¡ ë¼ë²¨ë“¤
    
    Returns:
        metrics: í‰ê°€ ì§€í‘œë“¤ ë”•ì…”ë„ˆë¦¬
    """
    from collections import Counter
    
    # ë¼ë²¨ì„ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì •ê·œí™”
    def normalize_labels(labels):
        unique_labels = sorted(set(labels))
        label_map = {old: new for new, old in enumerate(unique_labels)}
        return np.array([label_map[label] for label in labels])
    
    true_normalized = normalize_labels(true_labels)
    pred_normalized = normalize_labels(pred_labels)
    
    n_points = len(true_labels)
    
    # ì •í™•ë„ ê³„ì‚° (ë¼ë²¨ ë§¤ì¹­ ê³ ë ¤)
    # ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” ë¼ë²¨ ìŒì„ ì°¾ì•„ì„œ ë§¤ì¹­
    true_clusters = set(true_normalized)
    pred_clusters = set(pred_normalized)
    
    best_accuracy = 0.0
    
    # ëª¨ë“  ê°€ëŠ¥í•œ ë¼ë²¨ ë§¤í•‘ì„ ì‹œë„ (ì‘ì€ í´ëŸ¬ìŠ¤í„° ìˆ˜ì—ì„œë§Œ ê°€ëŠ¥)
    if len(pred_clusters) <= 10:
        import itertools
        for perm in itertools.permutations(pred_clusters):
            if len(perm) != len(true_clusters):
                continue
            
            mapped_pred = pred_normalized.copy()
            for i, pred_label in enumerate(perm):
                mapped_pred[pred_normalized == pred_label] = i
            
            accuracy = np.mean(true_normalized == mapped_pred)
            best_accuracy = max(best_accuracy, accuracy)
    
    # í´ëŸ¬ìŠ¤í„° ìˆœë„ (Purity) ê³„ì‚°
    total_correct = 0
    for pred_cluster in pred_clusters:
        cluster_mask = (pred_normalized == pred_cluster)
        if np.sum(cluster_mask) > 0:
            cluster_true_labels = true_normalized[cluster_mask]
            most_common_true_label = Counter(cluster_true_labels).most_common(1)[0][0]
            correct_in_cluster = np.sum(cluster_true_labels == most_common_true_label)
            total_correct += correct_in_cluster
    
    purity = total_correct / n_points
    
    return {
        'accuracy': best_accuracy,
        'purity': purity,
        'n_predicted_clusters': len(pred_clusters),
        'n_true_clusters': len(true_clusters)
    }

# Visualization
def visualize_clustering_result(pcd: o3d.geometry.PointCloud, 
                               labels: np.ndarray,
                               centroids: Optional[np.ndarray] = None,
                               title: str = "Clustering Result",
                               show_centroids: bool = True) -> None:
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
    
    Args:
        pcd: í¬ì¸íŠ¸ í´ë¼ìš°ë“œ
        labels: í´ëŸ¬ìŠ¤í„° ë¼ë²¨ë“¤
        centroids: í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì ë“¤ (ì„ íƒì‚¬í•­)
        title: ì°½ ì œëª©
        show_centroids: ì¤‘ì‹¬ì  í‘œì‹œ ì—¬ë¶€
    """
    geometries = []
    
    # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒ‰ìƒ ì„¤ì •
    pcd_copy = pcd.__copy__()
    points = np.array(pcd_copy.points)
    colors = np.zeros((len(points), 3))
    
    # ë¯¸ë¦¬ ì •ì˜ëœ ìƒ‰ìƒ íŒ”ë ˆíŠ¸
    color_palette = [
        [1, 0, 0],      # ë¹¨ê°•
        [0, 1, 0],      # ì´ˆë¡
        [0, 0, 1],      # íŒŒë‘
        [1, 1, 0],      # ë…¸ë‘
        [1, 0, 1],      # ë§ˆì  íƒ€
        [0, 1, 1],      # ì‹œì•ˆ
        [1, 0.5, 0],    # ì£¼í™©
        [0.5, 0, 1],    # ë³´ë¼
        [1, 0.5, 0.5],  # ë¶„í™
        [0.5, 1, 0.5],  # ì—°ì´ˆë¡
    ]
    
    unique_labels = np.unique(labels)
    
    for i, label in enumerate(unique_labels):
        mask = (labels == label)
        if label == -1:
            # ë…¸ì´ì¦ˆëŠ” íšŒìƒ‰ìœ¼ë¡œ í‘œì‹œ
            colors[mask] = [0.5, 0.5, 0.5]
        else:
            # í´ëŸ¬ìŠ¤í„°ëŠ” ì„œë¡œ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œ
            color_idx = label % len(color_palette)
            colors[mask] = color_palette[color_idx]
    
    pcd_copy.colors = o3d.utility.Vector3dVector(colors)
    geometries.append(pcd_copy)
    
    # ì¤‘ì‹¬ì  ì‹œê°í™”
    if centroids is not None and show_centroids:
        for i, centroid in enumerate(centroids):
            # ì¤‘ì‹¬ì ì„ êµ¬ë¡œ í‘œì‹œ
            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.2)
            sphere.translate(centroid)
            
            # ì¤‘ì‹¬ì  ìƒ‰ìƒ ì„¤ì •
            color_idx = i % len(color_palette)
            sphere.paint_uniform_color(color_palette[color_idx])
            sphere.compute_vertex_normals()
            geometries.append(sphere)
    
    # ì¢Œí‘œê³„ ì¶”ê°€
    coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=1.0)
    geometries.append(coordinate_frame)
    
    o3d.visualization.draw_geometries(geometries, window_name=title)